<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>K.L.</title>
  <icon>https://k-l-lambda.github.io/profile.png</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://k-l-lambda.github.io/"/>
  <updated>2025-04-23T16:01:47.649Z</updated>
  <id>https://k-l-lambda.github.io/</id>
  
  <author>
    <name>K.L.</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>打造音乐创作AI（二）</title>
    <link href="https://k-l-lambda.github.io/2025/04/23/notagen/"/>
    <id>https://k-l-lambda.github.io/2025/04/23/notagen/</id>
    <published>2025-04-23T23:59:09.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><img src="/images/symbolic-music-models2025.drawio.svg" /></picture><figcaption>截至2025年4月，主要的符号音乐算法作曲工作</figcaption></figure><p><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup><sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup><sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup><sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup><sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup><sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup><sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup><sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup></p><p>距离上一篇<a href="/2023/11/29/vae-based-music-encoder/">打造音乐创作AI（一）</a>已经过去一年多了，一直忙于工作和家庭，音乐算法研究进展甚微。最近看到了NotaGen<sup class="footnote-ref"><a href="#fn15" id="fnref15:1">[15:1]</a></sup>的发表，堪称符号算法作曲领域的里程碑式工作。不得不承认，自己还在半山腰磨蹭，别人已经把旗子插上山顶了。索性第二篇就介绍一下NotaGen，自己这边的思路以后再整理吧。</p><p>这篇由中央音乐学院与多所大学联合团队发表的工作，无论从算法路线和数据来源都做得非常完备。不仅发表了<a href="https://arxiv.org/pdf/2502.18008">论文</a>，也开放了<a href="https://github.com/ElectricAlexis/NotaGen">源代码</a>和<a href="https://electricalexis.github.io/notagen-demo/">Demo</a>。当然最宝贵的数据集并没有放出（可能是考虑版权问题）。</p><p>除了官方Demo，这里给出一些我自己使用NotaGen生成的键盘作品（曲谱图片可点开大图）：</p><figure><picture><audio src="/images/notagenx/20250409_162246.mp3" controls></audio><p><img src="/images/notagenx/20250409_162246_1.png" width="64px" /><img src="/images/notagenx/20250409_162246_2.png" width="64px" /><img src="/images/notagenx/20250409_162246_3.png" width="64px" /><img src="/images/notagenx/20250409_162246_4.png" width="64px" /></p></picture><figcaption>轻快的音阶跑动，旋律多变，没有过多重复</figcaption></figure><figure><picture><audio src="/images/notagenx/20250408_220949.mp3" controls></audio><p><img src="/images/notagenx/20250408_220949_1.png" width="64px" /><img src="/images/notagenx/20250408_220949_2.png" width="64px" /><img src="/images/notagenx/20250408_220949_3.png" width="64px" /><img src="/images/notagenx/20250408_220949_4.png" width="64px" /></p></picture><figcaption>李斯特式的富于想象力的奔放旋律，可惜曲式结构写成了古典主义的回旋曲式</figcaption></figure><figure><picture><audio src="/images/notagenx/20250408_221059.mp3" controls></audio><p><img src="/images/notagenx/20250408_221059_1.png" width="64px" /><img src="/images/notagenx/20250408_221059_2.png" width="64px" /><img src="/images/notagenx/20250408_221059_3.png" width="64px" /><img src="/images/notagenx/20250408_221059_4.png" width="64px" /><img src="/images/notagenx/20250408_221059_5.png" width="64px" /></p></picture><figcaption>诙谐、轻松的圆舞曲</figcaption></figure><figure><picture><audio src="/images/notagenx/20250408_222034.mp3" controls></audio><p><img src="/images/notagenx/20250408_222034_1.png" width="64px" /><img src="/images/notagenx/20250408_222034_2.png" width="64px" /><img src="/images/notagenx/20250408_222034_3.png" width="64px" /><img src="/images/notagenx/20250408_222034_4.png" width="64px" /></p></picture><figcaption>像一首练习曲（肖邦级难度的）</figcaption></figure><figure><picture><audio src="/images/notagenx/20250409_112628.mp3" controls></audio><p><img src="/images/notagenx/20250409_112628_1.png" width="64px" /><img src="/images/notagenx/20250409_112628_2.png" width="64px" /><img src="/images/notagenx/20250409_112628_3.png" width="64px" /><img src="/images/notagenx/20250409_112628_4.png" width="64px" /></p></picture><figcaption>一首有趣的AI作品，风格不落于俗套，bass声部居然用钢琴音色制造了一种打击乐的效果</figcaption></figure><p>依据paper和源代码这些可观察到的内容，以下从三个要点来简要介绍一下NotaGen的工作。</p><span id="more"></span><h2 id="Interleaved-ABC-Notation">Interleaved ABC Notation</h2><p>音乐作为一种时序模态，自回归模型始终是生成的首选方法。这种方法的本质是训练一个“文字接龙”式的语言模型，只需构造一种语言来描述目标模态（术语称为tokenize），剩下的工作就可以交给transformer<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup>等主干模型完成。如今，甚至连图像生成也逐渐向自回归模型靠拢，不过也有反其道而行之的尝试，<a href="https://arxiv.org/pdf/2502.05171">用扩散模型来处理自然语言</a>。</p><p>与音频生成任务不同，符号音乐生成的主要难度不在算法层面，而是数据表示和高质量的数据来源。</p><p>常见的五线谱数字化语言有MusicXML、MEI、ABC Notation、Lilypond、Humdrum这么几种。其中，MusicXML和MEI基于XML，文本冗余度较高。ABC Notation和Lilypond是专用语言，Lilypond的语法类似于Latex，灵活性较高，描述能力强但表达方式多变，不利于模型学习。笔者在<a href="/2023/11/29/vae-based-music-encoder/">上一篇</a>中介绍了自己基于Lilypond发明的变种语言<a href="https://github.com/findlab-org/paraff">Paraff</a>，此处不再赘述。剩下的ABC Notation和Humdrum两种语言都有人用来做符号音乐算法的数据表示。Humdrum更像是一种表格，而不太像一种语言，个人不太喜欢。</p><p>NotaGen使用的是<a href="https://notabc.app/abc/basics/">ABC Notation</a>的一种变种——Interleaved ABC Notation。Interleaved ABC Notation可能最早由MuPT<sup class="footnote-ref"><a href="#fn11" id="fnref11:1">[11:1]</a></sup>的作者提出，也可能是CLaMP 2<sup class="footnote-ref"><a href="#fn12" id="fnref12:1">[12:1]</a></sup>的作者<em>WU</em>独立设计的。毕竟思路很简单，即将曲谱从part-wise表示转换为time-wise表示。笔者的<a href="https://github.com/findlab-org/paraff">Paraff</a>也遵循同样的设计。见下图：</p><figure><picture><img src="/images/arxiv2410.13267-figure6.png" width="400px" /></picture><figcaption><p>Figure 6 from CLaMP 2图中以Chopin Op.9 No.2 前两小节为例</p><p>原始的ABC Notation是part-wise，或称voice-wise，从头到尾描述完一个声部再写下一个声部。<br/>而Interleaved ABC Notation是time-wise，完整描述一个小节的所有声部后再写下一个小节。</p></figcaption></figure><p>笔者之前没有采用ABC Notation的原因是，其自带的曲谱渲染器太过于简陋。而笔者的主要工作领域之一是OMR，需要高质量的曲谱图像数据。从生成数据的角度看，Lilypond显著优于ABC Notation。不过，NotaGen和CLaMP团队解决了ABC Notation到MusicXML的转换问题，并使用MuseScore作为曲谱渲染器及多种媒体格式的转换工具。这使得ABC Notation成为一个不错的选择，但对于复杂曲谱（如复调键盘作品），ABC Notation的表达能力仍有待观察。</p><p>MuseScore是商业级曲谱软件中唯一开源的，类似的方案还有<a href="https://www.verovio.org/">Verovio</a>。不过，当MuseScore开源时，笔者已经开发了自己的<a href="https://github.com/k-l-lambda/lotus">Lotus</a>项目。一旦转型需要进行大量额外的前端工作，因此没有深入使用。</p><h2 id="2-Levels-Decoder">2 Levels Decoder</h2><p>使用ABC Notation来做符号音乐生成，可能最早的是MuPT<sup class="footnote-ref"><a href="#fn11" id="fnref11:2">[11:2]</a></sup>。查看其论文，除了数据表示外，几乎没有太多关于音乐方面的内容，主要探讨的是数据集相关的Scaling Law。给我类似印象的还有Music Transformer<sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2:1]</a></sup>，除了实现了MIDI数据的tokenization，其他主要是提出了一种优化transformer二次方复杂度的方法。不过，NotaGen团队在模型架构方面显然更有追求。确切地说，是第二作者（或者应该称为同等贡献作者之一）<em>Shangda Wu</em>再次应用了他之前提出的一种两级解码架构。</p><figure><picture><img src="/images/arxiv2301.02884-figure1.png" width="480px" /></picture><figcaption>Figure 1 from TunesFormer这里是能找到的WU的2 levels decoder架构的最早出处</figcaption></figure><p><sup class="footnote-ref"><a href="#fn5" id="fnref5:1">[5:1]</a></sup></p><figure><picture><img src="/images/arxiv2502.18008-figure2.png" width="480px" /></picture><figcaption>Figure 2 from NotaGen</figcaption></figure><p><sup class="footnote-ref"><a href="#fn15" id="fnref15:2">[15:2]</a></sup></p><p>笔者第一次看到这里马上联想到之前的一篇MegaByte<sup class="footnote-ref"><a href="#fn7" id="fnref7:1">[7:1]</a></sup>:</p><figure><picture><img src="/images/arxiv2305.07185-figure1.png" width="480px" /></picture><figcaption>Figure 1 from MegaByte</figcaption></figure><p><sup class="footnote-ref"><a href="#fn7" id="fnref7:2">[7:2]</a></sup></p><p>然而查看发表日期，<em>WU</em>的<em>TunesFormer</em><sup class="footnote-ref"><a href="#fn5" id="fnref5:2">[5:2]</a></sup>还是在MegaByte<sup class="footnote-ref"><a href="#fn7" id="fnref7:3">[7:3]</a></sup>之前。但是在bGPT<sup class="footnote-ref"><a href="#fn10" id="fnref10:1">[10:1]</a></sup>的论文中，作者又亲自提到是受了MegaByte的启发。所以究竟是独立提出的想法还是借鉴了更早的工作，笔者还无法下定论。</p><p>两级解码器的思路来源是清晰的，其目的是缓解注意力机制中的序列长度问题。即使按照<em>MuPT</em><sup class="footnote-ref"><a href="#fn11" id="fnref11:3">[11:3]</a></sup>的路线，有了目前推理加速领域的技术支持（如FlashAttention），一首音乐的长度似乎也不再是问题。然而在符号音乐领域，有一个天然可以利用的结构——音乐中周期性的强弱节奏构成的小节——使得符号音乐在时间轴上天生具有二级结构，不加以利用就显得浪费了。</p><p>笔者在<a href="/2023/11/29/vae-based-music-encoder/">前一篇</a>中构造的单小节曲谱编/解码器也是基于类似的思路。不过这里的区别在于，笔者之前的设计将网络参数的重点放在token这一级（对应NotaGen的byte level），而bar-level在最终解码时仅作为前情提要式的信息辅助。训练时，bar-level的小节编码器只关注单个小节，并作为token-level训练的预训练模块加载。而在NotaGen中，bar-level解码器才是作曲的主角，它把整个小节当成一个“token”来处理。这里打引号的token其实是潜在空间中的一个向量，一小节内容岂是一个词能概括的。不过<a href="/2023/11/29/vae-based-music-encoder/">前一篇</a>中笔者的工作也证明了，一小节音乐内容的确能够压缩到一个几百维的向量中。然后byte-level解码器仅关注单个小节信息，完全不看上下文。缩小关注范围带来的另一个好处是，连BPE tokenizer也省去了。</p><p>这里的关键在于，虽然两级联合训练的思路并不难想到，但这样做其实是有代价的。在权衡各种解法利弊时，这种方法会被过早淘汰出关注视野。就像下棋一样，有时最优的一步会出现在看似不可能的地方。这里的代价是，为了高效地同时训练两级解码器，byte-level的输入数据长度需要强制对齐，这会牺牲一部分很长的小节（例如一长串快速短音符或连续多组复杂和弦）。从结果来看，NotaGen的设计很可能是做出了正确的取舍。</p><p>顺便一提，这个两级解码器结构在<em>WU</em>的工作中更多用于音乐信息检索(MIR)领域，见于<em>CLaMP</em> 1-3<sup class="footnote-ref"><a href="#fn6" id="fnref6:1">[6:1]</a></sup><sup class="footnote-ref"><a href="#fn12" id="fnref12:2">[12:2]</a></sup><sup class="footnote-ref"><a href="#fn14" id="fnref14:1">[14:1]</a></sup>。在<em>bGPT</em><sup class="footnote-ref"><a href="#fn10" id="fnref10:2">[10:2]</a></sup>中，还尝试了使用该架构生成图像、音频以及CPU状态预测。</p><h2 id="CLaMP-DPO">CLaMP-DPO</h2><p>在模型训练方面，NotaGen也沿袭了目前LLM主流的pretrain-finetune-RL的路线范式。</p><p>从源代码来看，每个阶段都是全量参数的训练，没有使用LoRA等附加参数方法。除了数据集和损失函数的变化，不同阶段仅调整了学习率。</p><p>值得说一下的是强化学习阶段，作者复用了自己之前的工作CLaMP 2<sup class="footnote-ref"><a href="#fn12" id="fnref12:3">[12:3]</a></sup>作为评估模型。CLaMP 2原本用于MIR任务，提取每首曲子的音乐特征。在这里相当于RLHF中的打分模型，用于比较finetune后模型生成的作品与相同提示下参考作品在语义特征上的相似度。在生成时，使用的提示是一个三元组：（时代，作曲家，体裁）。在每一种提示组合下，把所有生成作品按最优相似度排序，其中前10%进入接受集，末尾10%进入拒绝集。训练时，从接受集和拒绝集中各采样一个样本组成正负样本对$(pw, pl)$，然后计算DPO-Positive<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>损失函数：</p>$$\mathcal{L}_{\text{DPOP}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(p, x_{pw}, x_{pl}) \sim \mathcal{D}} \left[ \log \sigma \left( \underbrace{ \beta \log \frac{\pi_\theta(x_{pw} | p)}{\pi_{\text{ref}}(x_{pw} | p)} - \beta \log \frac{\pi_\theta(x_{pl} | p)}{\pi_{\text{ref}}(x_{pl} | p)} }_{\text{DPO items}} - \underbrace{ \beta \lambda \cdot \max \left( 0, \log \frac{\pi_{\text{ref}}(x_{pw} | p)}{\pi_\theta(x_{pw} | p)} \right) }_{\text{DPOP item}} \right) \right]$$<p><sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup><sup class="footnote-ref"><a href="#fn16" id="fnref16:1">[16:1]</a></sup></p><p>补充说两句，笔者在算法开发中有一种体会，分析模型和生成模型的训练是相辅相成的。这里的分析模型是指输入数据的熵高于输出数据的模型，生成模型则相反。分析模型在于排除噪声干扰，识别数据中存在的某种潜在模式，而生成模型则通过混入噪声（或从噪声出发），并引向目标分布内的多样性维度。笔者最初训练曲谱生成模型是为了作为OMR的数据来源，通过生成符合真实分布的曲谱数据来合成有标注的曲谱图像。反之，更好地识别曲谱中的各种模式则有利于条件化地控制音乐生成。</p><h1>其他</h1><p>除了以上三点，想必还有大量工作隐藏在数据集的制备中，包括清洗、格式转换工具开发等。尤其是其中数据来源表格中的<em>Internal Sources</em>一项，想必是中央音乐学院得天独厚的资源。就算你穷尽其他开放来源也才凑齐不到人家三成！</p><table><thead><tr><th>Data Sources</th><th style="text-align:right">Amount</th></tr></thead><tbody><tr><td><em>DCML Corpora</em></td><td style="text-align:right">560</td></tr><tr><td><em>OpenScore String Quartet Corpus</em></td><td style="text-align:right">342</td></tr><tr><td><em>OpenScore Lieder Corpus</em></td><td style="text-align:right">1,334</td></tr><tr><td><em>ATEPP</em></td><td style="text-align:right">55</td></tr><tr><td><em>KernScores</em></td><td style="text-align:right">221</td></tr><tr><td><em>Internal Sources</em></td><td style="text-align:right">6,436</td></tr><tr><td><strong>Total</strong></td><td style="text-align:right"><strong>8,948</strong></td></tr></tbody></table><figcaption>Table 1 from <em>arxiv2502.18008</em>: Data sources and the respective amounts for fine-tuning</figcaption><p><sup class="footnote-ref"><a href="#fn15" id="fnref15:3">[15:3]</a></sup></p><p>当然，尽管作为里程碑之作，NotaGen目前仍存在一些不足之处。一是伴奏声部的编曲整体来看偏于简单，缺乏浪漫主义时期那种复杂多声部的丰富编排。这一点从听感上可能不太明显，但对于基于五线谱的算法作曲方向来说，清晰的声部逻辑正是该方向追求的特色之一。</p><p>另外，NotaGen模型还可以观察到一定程度的过拟合现象。也就是说，它可能直接记住了训练集中的部分曲谱。例如以下这两首：</p><figure><audio src="/images/notagenx/20250408_213249.mp3" controls></audio><figcaption>开头明显“剽窃”了舒伯特的<a href="https://www.youtube.com/watch?v=0XlUsEfcKB0" target="_blank">中速的快板</a>，后面做了有趣的变奏</figcaption></figure><figure><audio src="/images/notagenx/20250409_163916.mp3" controls></audio><figcaption>这一首几乎完整复现了柴可夫斯基《天鹅湖》的旋律</figcaption></figure><p>过拟合有时可以被视为一种积极的信号，意味着通过继续扩大数据集可以获得更多收益。目前，一个潜在的曲谱数据来源是国际音乐库<a href="https://imslp.org">IMSLP</a>。有朝一日能够将IMSLP上近7万部名家名作的PDF曲谱数字化，可用的数据量至少还能增加一到两个数量级。</p><hr><p>References:</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Transformer: <a href="https://arxiv.org/abs/1706.03762">arxiv1706.03762</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Music Transformer: <a href="https://arxiv.org/abs/1809.04281">arxiv1809.04281</a> <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>MusicBERT: <a href="https://arxiv.org/abs/2106.05630">arxiv2106.05630</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Deep Choir: <a href="https://arxiv.org/abs/2202.08423">arxiv2202.08423</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>TunesFormer: <a href="https://arxiv.org/abs/2301.02884">arxiv2301.02884</a> <a href="#fnref5" class="footnote-backref">↩︎</a> <a href="#fnref5:1" class="footnote-backref">↩︎</a> <a href="#fnref5:2" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>CLaMP: <a href="https://arxiv.org/abs/2304.11029">arxiv2304.11029</a> <a href="#fnref6" class="footnote-backref">↩︎</a> <a href="#fnref6:1" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>MegaByte: <a href="https://arxiv.org/abs/2305.07185">arxiv2305.07185</a> <a href="#fnref7" class="footnote-backref">↩︎</a> <a href="#fnref7:1" class="footnote-backref">↩︎</a> <a href="#fnref7:2" class="footnote-backref">↩︎</a> <a href="#fnref7:3" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Mamba: <a href="https://arxiv.org/abs/2312.00752">arxiv2312.00752</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>MambaByte: <a href="https://arxiv.org/abs/2401.13660">arxiv2401.13660</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>bGPT: <a href="https://arxiv.org/abs/2402.19155">arxiv2402.19155</a> <a href="#fnref10" class="footnote-backref">↩︎</a> <a href="#fnref10:1" class="footnote-backref">↩︎</a> <a href="#fnref10:2" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p>MuPT: <a href="https://arxiv.org/abs/2404.06393">arxiv2404.06393</a> <a href="#fnref11" class="footnote-backref">↩︎</a> <a href="#fnref11:1" class="footnote-backref">↩︎</a> <a href="#fnref11:2" class="footnote-backref">↩︎</a> <a href="#fnref11:3" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p>CLaMP 2: <a href="https://arxiv.org/abs/2410.13267">arxiv2410.13267</a> <a href="#fnref12" class="footnote-backref">↩︎</a> <a href="#fnref12:1" class="footnote-backref">↩︎</a> <a href="#fnref12:2" class="footnote-backref">↩︎</a> <a href="#fnref12:3" class="footnote-backref">↩︎</a></p></li><li id="fn13" class="footnote-item"><p>Music Event Transformer (MET): <a href="https://github.com/SkyTNT/midi-model">https://github.com/SkyTNT/midi-model</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p></li><li id="fn14" class="footnote-item"><p>CLaMP 3: <a href="https://arxiv.org/abs/2502.10362">arxiv2502.10362</a> <a href="#fnref14" class="footnote-backref">↩︎</a> <a href="#fnref14:1" class="footnote-backref">↩︎</a></p></li><li id="fn15" class="footnote-item"><p>NotaGen: <a href="https://arxiv.org/abs/2502.18008">arxiv2502.18008</a> <a href="#fnref15" class="footnote-backref">↩︎</a> <a href="#fnref15:1" class="footnote-backref">↩︎</a> <a href="#fnref15:2" class="footnote-backref">↩︎</a> <a href="#fnref15:3" class="footnote-backref">↩︎</a></p></li><li id="fn16" class="footnote-item"><p>DPO-Positive: <a href="https://arxiv.org/abs/2402.13228">arxiv2402.13228</a> <a href="#fnref16" class="footnote-backref">↩︎</a> <a href="#fnref16:1" class="footnote-backref">↩︎</a></p></li><li id="fn17" class="footnote-item"><p>DPO: <a href="https://arxiv.org/abs/2305.18290">arxiv2305.18290</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/symbolic-music-models2025.drawio.svg&quot; /&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		截至2025年4月，主要的符号音乐算法作曲工作
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn1&quot; id=&quot;fnref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn2&quot; id=&quot;fnref2&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn3&quot; id=&quot;fnref3&quot;&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn4&quot; id=&quot;fnref4&quot;&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn5&quot; id=&quot;fnref5&quot;&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn6&quot; id=&quot;fnref6&quot;&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn7&quot; id=&quot;fnref7&quot;&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn8&quot; id=&quot;fnref8&quot;&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn9&quot; id=&quot;fnref9&quot;&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn10&quot; id=&quot;fnref10&quot;&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn11&quot; id=&quot;fnref11&quot;&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn12&quot; id=&quot;fnref12&quot;&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn13&quot; id=&quot;fnref13&quot;&gt;[13]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn14&quot; id=&quot;fnref14&quot;&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn15&quot; id=&quot;fnref15&quot;&gt;[15]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;距离上一篇&lt;a href=&quot;/2023/11/29/vae-based-music-encoder/&quot;&gt;打造音乐创作AI（一）&lt;/a&gt;已经过去一年多了，
一直忙于工作和家庭，音乐算法研究进展甚微。
最近看到了NotaGen&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn15&quot; id=&quot;fnref15:1&quot;&gt;[15:1]&lt;/a&gt;&lt;/sup&gt;的发表，堪称符号算法作曲领域的里程碑式工作。
不得不承认，自己还在半山腰磨蹭，别人已经把旗子插上山顶了。
索性第二篇就介绍一下NotaGen，自己这边的思路以后再整理吧。&lt;/p&gt;
&lt;p&gt;这篇由中央音乐学院与多所大学联合团队发表的工作，无论从算法路线和数据来源都做得非常完备。
不仅发表了&lt;a href=&quot;https://arxiv.org/pdf/2502.18008&quot;&gt;论文&lt;/a&gt;，也开放了&lt;a href=&quot;https://github.com/ElectricAlexis/NotaGen&quot;&gt;源代码&lt;/a&gt;和&lt;a href=&quot;https://electricalexis.github.io/notagen-demo/&quot;&gt;Demo&lt;/a&gt;。
当然最宝贵的数据集并没有放出（可能是考虑版权问题）。&lt;/p&gt;
&lt;p&gt;除了官方Demo，这里给出一些我自己使用NotaGen生成的键盘作品（曲谱图片可点开大图）：&lt;/p&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;audio src=&quot;/images/notagenx/20250409_162246.mp3&quot; controls&gt;&lt;/audio&gt;
		&lt;p&gt;
			&lt;img src=&quot;/images/notagenx/20250409_162246_1.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250409_162246_2.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250409_162246_3.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250409_162246_4.png&quot; width=&quot;64px&quot; /&gt;
		&lt;/p&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		轻快的音阶跑动，旋律多变，没有过多重复
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;audio src=&quot;/images/notagenx/20250408_220949.mp3&quot; controls&gt;&lt;/audio&gt;
		&lt;p&gt;
			&lt;img src=&quot;/images/notagenx/20250408_220949_1.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_220949_2.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_220949_3.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_220949_4.png&quot; width=&quot;64px&quot; /&gt;
		&lt;/p&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		李斯特式的富于想象力的奔放旋律，可惜曲式结构写成了古典主义的回旋曲式
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;audio src=&quot;/images/notagenx/20250408_221059.mp3&quot; controls&gt;&lt;/audio&gt;
		&lt;p&gt;
			&lt;img src=&quot;/images/notagenx/20250408_221059_1.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_221059_2.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_221059_3.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_221059_4.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_221059_5.png&quot; width=&quot;64px&quot; /&gt;
		&lt;/p&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		诙谐、轻松的圆舞曲
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;audio src=&quot;/images/notagenx/20250408_222034.mp3&quot; controls&gt;&lt;/audio&gt;
		&lt;p&gt;
			&lt;img src=&quot;/images/notagenx/20250408_222034_1.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_222034_2.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_222034_3.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250408_222034_4.png&quot; width=&quot;64px&quot; /&gt;
		&lt;/p&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		像一首练习曲（肖邦级难度的）
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;audio src=&quot;/images/notagenx/20250409_112628.mp3&quot; controls&gt;&lt;/audio&gt;
		&lt;p&gt;
			&lt;img src=&quot;/images/notagenx/20250409_112628_1.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250409_112628_2.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250409_112628_3.png&quot; width=&quot;64px&quot; /&gt;
			&lt;img src=&quot;/images/notagenx/20250409_112628_4.png&quot; width=&quot;64px&quot; /&gt;
		&lt;/p&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		一首有趣的AI作品，风格不落于俗套，bass声部居然用钢琴音色制造了一种打击乐的效果
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;依据paper和源代码这些可观察到的内容，以下从三个要点来简要介绍一下NotaGen的工作。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="deep_learning" scheme="https://k-l-lambda.github.io/tags/deep-learning/"/>
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="music_algorithm" scheme="https://k-l-lambda.github.io/tags/music-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>怎样用Github写日记</title>
    <link href="https://k-l-lambda.github.io/2025/01/03/diary-on-github/"/>
    <id>https://k-l-lambda.github.io/2025/01/03/diary-on-github/</id>
    <published>2025-01-03T23:15:41.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，写博客或日记有两种方式，一种是使用Web2.0时代的<a href="https://zh.wikipedia.org/wiki/%E7%94%A8%E6%88%B7%E7%94%9F%E6%88%90%E5%86%85%E5%AE%B9">UGC</a>平台网站（如豆瓣、新浪博客等），另一种是自己搭建个人网站。第一种选择门槛低，可以立即开始写作，但缺点是无法控制所有细节，对富文本格式（尤其是程序员喜欢的<a href="https://markdown.com.cn/intro.html#markdown-%E6%98%AF%E4%BB%80%E4%B9%88">Markdown</a>）支持有限。第二种选择，如<a href="https://hexo.io/zh-cn/">Hexo</a>，则需要折腾一些代码，配置大量细节选项，写博客如同开发一个小型项目。如果只是作为个人备忘性质的日志，就过于繁琐了。</p><p>本文提供了一种新的折衷思路，即直接把Github当作UGC平台，兼取两者优点，可谓懒人良方。</p><figure><picture><img src="/images/github-diary-demo.png" /></picture><figcaption>Github作为一个极佳的写作网站<a href="https://github.com/k-l-lambda/diary-one" target="_blank">diary-one</a></figcaption></figure><span id="more"></span><p>看到这里你可能已经猜到了，无非就是建一个叫做“我的日记”的GitHub仓库，然后每天以日期作文件名，新建一个Markdown文件，记录当天事项，最后整个仓库变成一个文档合集。然而想把这群文档维护得好用，还是可以下点功夫的。</p><p>首先，可以给日记建个日历索引视图，用来快速定位到某一天。把这个视图直接放在项目的 <code>README</code> 文件中，这样从仓库主页就能看到。就像这样：</p><figure><picture><img src="/images/github-diary-calendar.png" width="458" /></picture><figcaption>日历视图</figcaption></figure><p>鼠标悬停在某一天，还能看到当天的日记大纲（即#开头的那些title内容）。</p><p>自动化<a href="https://github.com/k-l-lambda/diary-one/blob/main/tools/buildCalendar.js">生成日历视图逻辑</a>非常简单，用不了100行代码。</p><p>然后是同类内容的聚合整理。比如说每日记录的读书笔记、论文阅读、背单词的生词本、健身记录、乐器练习记录等等，每种内容都聚合整理到一个单独的文件中，方便查阅。例如：</p><figure><picture><img src="/images/github-diary-vocab.png" width="465" /></picture><figcaption>生词表</figcaption></figure><figure><picture><img src="/images/github-diary-reading.png" width="482" /></picture><figcaption>读书笔记汇总</figcaption></figure><figure><picture><img src="/images/github-diary-arxiv.png" width="480" /></picture><figcaption>arxiv论文列表</figcaption></figure><p>这部分我写了一些脚本插件，小伙伴们可以根据需要自行<a href="https://github.com/k-l-lambda/diary-one/blob/main/tools/buildStatistics.js">开启</a>，或者自己扩展一些新的脚本。脚本的原理很简单，就是捕获日记内容中的一些固定模式，比如二级标题<code>##</code>后面的内容中带有书名号<code>《》</code>的，就认为是读书笔记，然后把标题之后的段落摘取出来，同一本书的笔记就放到一起。每个脚本会在<a href="https://github.com/k-l-lambda/diary-one/tree/main/memo">memo</a>目录下创建一个单独的聚合条目，完全自动化维护。</p><p>另外，为了方便连续地翻看日记，我加了一个<a href="https://github.com/k-l-lambda/diary-one/blob/main/tools/cowriter.js#L11">导航栏功能</a>，每天会自动给新日记加上前后链接。考虑到有人觉得每天手工建一个新文件也很麻烦，<a href="https://github.com/k-l-lambda/diary-one/blob/main/tools/cowriter.js#L5">这里</a>还有一个每日自动创建日记文件的可选功能，新文件默认内容效果大约是这样的：</p><blockquote><h1>🐷</h1><p>我今天犯了个懒，啥也没写……</p></blockquote><p>如果哪天没写日记，这个可爱的“猪猪声明”就会作为当天的系统默认记录。</p><p>现在如果你在琢磨这么多脚本要怎么运行，呵呵，不存在的！它们都是由<a href="https://github.com/k-l-lambda/diary-one/actions">Github Actions</a>在后台自动触发的。注意日历视图上方有三个action徽章，只要它们是绿色的就说明一切运转良好。就是这么轻松。</p><p>欢迎小伙伴们Fork我这个<a href="https://github.com/k-l-lambda/diary-one">示例仓库</a>。这个仓库有两个用意，其<a href="https://github.com/k-l-lambda/diary-one/tree/initial">initial</a>分支是一个空白的日记模板，Fork之后可以直接拿来用；main分支作为示例，同时也当作所有人的涂鸦墙，谁想记点什么都可以提交PR，或直接成为项目成员，欢迎加入。</p><p>余生短暂，最后借用《黑暗森林》和帕斯卡的名言，记忆就像一把沙子，自以为抓的很牢，其实早就从指缝间溜走了，让我们给岁月以文明，给时光以日记。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;众所周知，写博客或日记有两种方式，一种是使用Web2.0时代的&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E7%94%A8%E6%88%B7%E7%94%9F%E6%88%90%E5%86%85%E5%AE%B9&quot;&gt;UGC&lt;/a&gt;平台网站（如豆瓣、新浪博客等），另一种是自己搭建个人网站。
第一种选择门槛低，可以立即开始写作，但缺点是无法控制所有细节，对富文本格式（尤其是程序员喜欢的&lt;a href=&quot;https://markdown.com.cn/intro.html#markdown-%E6%98%AF%E4%BB%80%E4%B9%88&quot;&gt;Markdown&lt;/a&gt;）支持有限。
第二种选择，如&lt;a href=&quot;https://hexo.io/zh-cn/&quot;&gt;Hexo&lt;/a&gt;，则需要折腾一些代码，配置大量细节选项，写博客如同开发一个小型项目。如果只是作为个人备忘性质的日志，就过于繁琐了。&lt;/p&gt;
&lt;p&gt;本文提供了一种新的折衷思路，即直接把Github当作UGC平台，兼取两者优点，可谓懒人良方。&lt;/p&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/github-diary-demo.png&quot; /&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		Github作为一个极佳的写作网站
		&lt;a href=&quot;https://github.com/k-l-lambda/diary-one&quot; target=&quot;_blank&quot;&gt;diary-one&lt;/a&gt;
	&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
    
      <category term="essay" scheme="https://k-l-lambda.github.io/tags/essay/"/>
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="promotion" scheme="https://k-l-lambda.github.io/tags/promotion/"/>
    
  </entry>
  
  <entry>
    <title>有向图模型的三要素分析法</title>
    <link href="https://k-l-lambda.github.io/2023/12/10/three-elements-diagram/"/>
    <id>https://k-l-lambda.github.io/2023/12/10/three-elements-diagram/</id>
    <published>2023-12-10T15:58:57.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是笔者多年前刚刚接触深度学习时的一些思考，最近才抽空整理出来，顺便也引用了一些近年新出现的网络结构作为例子。希望能给同行带来一些有益的启发。</p><p>早些年读深度学习论文的时候经常有一种感觉，作者已经在深入描述网络结构的细节，但笔者自己连其应用环境的大致图景还没有清晰概念。固然有对其前置工作了解不足的因素，但笔者认为还有一个重要原因是深度学习领域还缺少一种成熟的建模图示标准，类似UML。一个统一直观的图示系统应该有力地体现出神经网络的一些通用的基本要素，并对建模问题分析有辅助作用。本文可以视为笔者针对有向图模型通用图示的一个尝试性的提议。</p><p>首先我们尝试从一个简单模型结构入手。</p><h2 id="简单分类模型">简单分类模型</h2><p>考虑一个拍照识物应用<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，我们可以将其建模为一个简单的分类模型，用数学语言描述，我们的目标是想获得这样一个函数：</p><p>$$y’=f_\theta(x)$$</p><p>其中x代表输入的图像，y’是模型输出的类别概率向量。函数f是我们建模的一套计算图（譬如CNN网络），其下标θ代表模型权重，可以理解为计算图中待填入的知识。下文中笔者将计算图函数均写作二元函数，即接受权重和输入数据两个参数。</p><p>模型的训练是一个数值优化问题，通常记作如下表达式的求解：</p><p>$$\arg \min_\theta { \mathbb{E}_{(x,y)\sim (X,Y)} [l(f(\theta, x), y)] }$$</p><p>其中(X, Y)是一个有标注数据集，即特征-标签（图像-类别）集合。小写的x,y是来自数据集中的 (特征, 标签) 采样。$l$是损失函数，是某种度量<em>模型输出值与标签的距离</em>的函数，针对不同模态的数据可以有不同的选择。argmin代表求解某个θ的取值，使得右侧的期望表达式达成最小化。</p><p>数学表达式晦涩难读，但却是理解很多论文的最佳途径，因为其精确给出了问题的全景。有没有更直观的表达方式？针对上式笔者脑补了如下的图形：</p><figure><picture><img src="/images/diagram-simple-classification.drawio.svg" /></picture><figcaption>简单分类模型图示</figcaption></figure><p>其中竖着写的~是数学中的服从分布符号，可以理解为从数据集（或某种先验分布）中采集一个样本。</p><p>f上的*代表其中具有<strong>可训练参数</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>，即权重θ。这里其实笔者可以使用颜色来标注，但为了与平时在草纸上手写时的记号习惯保持一致，仍保留了星号。</p><p>圆圈中的ce即损失函数$l$，这里采用分类任务常用的交叉熵(Cross Entropy)。</p><p><em>loss</em>左侧的尖朝下三角形▽代表<strong>优化目标</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2:1]</a></sup>为最小化loss的值。</p><p>整个计算图的方向为由上至下，与很多paper作者采纳的习惯相反。这是出于以下考虑：<strong>数据</strong>是绝大多数深度学习任务的基础，因此分析建模问题时我们可以先列出可资利用的各种数据资源。（按自然的方式）在草稿上把所有数据项列在最上一行，然后从上至下画出可能的模型结构。</p><p>关于模块的形状，笔者考虑把数值模块和纯解析模块区分开来，纯解析模块用圆圈表示，提示其中不含权重（不论是当前训练中的还是来自预训练）。在笔者想象中，纯解析计算像是一种柏拉图实在（不似尘世之物），而数据集和权重则来自物理实在，充满各种细节和随机性。</p><p>如果是由我们自己来搭建一个针对某个任务的模型计算图，我们只要掌握其中的要点就可以确保做出一个实际可行的方案。<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>为了看出这其中的要点在哪里（同时也为了不失一般性），下面笔者简化了刚才的数学表达式。由于X, Y都是数据，可以合并为一个符号X来表示；而$f$, $l$都是计算图中的函数，可以合并为函数$g$，于是上式改写成：</p><p>$$\arg \min_\theta { \mathbb{E}_{x\sim X} [ g(\theta, x) ] }$$</p><p>我们把右侧的期望表达式记为优化目标A：</p><p>$$A = \mathbb{E}_{x\sim X} [ g(\theta, x) ]$$</p><p>刚才说了，模型训练是一个数值优化过程。具体说是把计算图中每个环节对训练权重θ求导，然后反向传播梯度，这个过程可以（略为粗糙地）记作下式：</p><p>$$\theta \gets  \theta - \gamma  \cdot \frac{\mathrm{d} }{\mathrm{d} \theta} A$$</p><p>其中$\gamma$是学习率，对我们的讨论不重要。</p><p>这里$g$就是我们想要搭建的计算图，它的组成部分我们暂时可以当作黑箱。现在我们可以看出，<strong>约束计算图g的三个要素是数据集X、训练权重θ和优化目标A</strong>。这是因为X和θ都是$g$的参数，而A决定了θ怎样得到更新。</p><p>这里还有一个重要的观察是，虽然计算图是从上至下的，但优化过程里只有θ是变量，所以信息的流动方向其实是从X流向θ。因此，有向图模型的范式可以概括为：凭借<strong>目标A</strong>，信息由<strong>数据X</strong>注入<strong>权重θ</strong>，以此获得智能。这里的“智能”指的是某种信息编码/解码能力，类似于人类处理问题的快速反应能力，或者叫直觉。</p><p>以下我们使用三要素分析方法，来考察一下各种常见的有向图模型结构。这里我们仍把计算图中各模块的细节当作黑箱，只关注大框架上各种模型与数据之间的作用方式。</p><span id="more"></span><h2 id="生成式对抗网络-GAN">生成式对抗网络 GAN</h2><p>GAN有两个模块：生成器g和辨别器d，训练时需要交替训练两个模块。这个过程可以简明地由下图概括：</p><figure><picture><img src="/images/diagram-gan.drawio.svg" /></picture><figcaption>经典生成式对抗网络图示，训练中(a) (b)两个阶段交替进行</figcaption></figure><p>星号标在哪里，可训练权重就在哪，除此以外计算图里其他的地方都是冻结的。</p><p>注意图中三角形的方向，可见两个阶段交替时除了训练的权重位置发生了变化，优化目标也发生了反转。这正是“对抗”的含义。</p><p>值得注意的是，这里只有X是数据集，没有标注（因此GAN属于无监督学习）。除此之外出现了一个来自正态分布的隐变量z，这是因为GAN假定数据中存在某种隐含的规律，但我们不知道它是什么，所以采用最大熵原则将其建模为𝒩(0, 1)。</p><h2 id="生成网络的反向投影">生成网络的反向投影</h2><p>如果你认为星号只能出现在某个方块前，那么你错了。</p><p>这是一个有意思的反例。假定我们现在已经有了一个无条件生成模型g（譬如就是来自刚才GAN里面的那个g），g把任意一个噪声编码z转换成符合原始数据集X分布的样本x’。然而现在我们想通过一个给定样本x，去反向得到它对应的编码z。这就是生成网络的反向投影，其图示如下：</p><figure><picture><img src="/images/diagram-gen-projector.drawio.svg" /></picture><figcaption>生成网络的反向投影</figcaption></figure><p><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p><p>梯度反向传播可以直接作用到输入数据z’上。由此可以看出，当我们写出二元函数$g(\theta, x)$时，两个参数权重$\theta$和数据$x$的地位是真正平等的。而我们平时思考中往往会忽视这一点。</p><p>另外早期的<a href="https://github.com/xunhuang1995/AdaIN-style">风格迁移</a>模型也是利用了类似的方法。</p><h2 id="变分自动编码器-VAE">变分自动编码器 VAE</h2><figure><picture><img src="/images/diagram-vae.drawio.svg" /></picture><figcaption>变分自动编码器VAE图示</figcaption></figure><p>其中<em>rp</em>代表<a href="https://en.wikipedia.org/wiki/Variational_autoencoder#Reparameterization">reparameterization</a>，<em>KL</em>处是计算$\mathcal N(\mu, \sigma^2)$与正态分布之间的KL散度：</p><p>$$KL (\mathcal N(\mu, \sigma^2) \left | \right | \mathcal N(0, 1)) = \frac{1}{2} (-\log \sigma^2 + \mu^2 + \sigma ^2 - 1)$$</p><h2 id="自回归模型-Autoregression">自回归模型 Autoregression</h2><p><a href="https://zh.wikipedia.org/zh-cn/%E8%87%AA%E6%88%91%E8%BF%B4%E6%AD%B8%E6%A8%A1%E5%9E%8B">自回归模型</a>用来迭代地生成一个序列。</p><figure><picture><img src="/images/diagram-autoregression.drawio.svg" /></picture><figcaption>自回归模型图示</figcaption></figure><p>可见自回归模型的训练跟一个普通的分类模型没有太大区别。巧妙之处在于，利用了序列元素之间的转移概率，输入和输出的数据都从同一个序列上截取获得。</p><p>然后是条件化自回归模型：</p><figure><picture><img src="/images/diagram-autoregression-conditional.drawio.svg" /></picture><figcaption>条件化自回归模型图示</figcaption></figure><p>通常用作两种语言之间的翻译模型，也可以是其他模态转换成序列，如<a href="https://paperswithcode.com/task/image-captioning">captioning</a>任务。无论是<a href="https://dataxujing.github.io/seq2seqlearn/chapter1/">seq2seq</a>还是transformer encoder+decoder都可以概括为这种结构。</p><h2 id="扩散模型-Diffusion">扩散模型 Diffusion</h2><p>扩散模型的直观理解可以参考笔者之前的<a href="/2023/04/22/diffusion-model-illustration/">post</a>。其复杂更多在于推测阶段的迭代过程，模型训练本身反倒是比较容易说明，如图：</p><figure><picture><img src="/images/diagram-diffusion.drawio.svg" /></picture><figcaption>扩散模型图示</figcaption></figure><p>直白地说，模型是用来从一个被噪声污染样本里鉴别出噪声信号。</p><p>我们平时常见的文生图、文生视频模型是条件化的扩散模型，结构也差不多：</p><figure><picture><img src="/images/diagram-diffusion-conditional.drawio.svg" /></picture><figcaption>条件化扩散模型图示</figcaption></figure><p>这里C是生成样本X对应的一个提示，C和X共享部分相同的信息内容，但模态或编码形式不同。</p><h2 id="Transformer的单层结构分析">Transformer的单层结构分析</h2><p>三要素分析更多是描述计算图的大框架，通常在分析某一模块的微观细节上帮助不大。不过我们不妨来分析一下当前热门的大语言模型的基础模块，以下图示描述transformer单层编码器结构：</p><figure><picture><img src="/images/diagrom-transformer-layer.drawio.svg" /></picture><figcaption>Transformer Encoder Layer图示</figcaption></figure><p>这里有个重要的观察是，transformer的核心机制“多头注意力”和“缩放点乘注意力”其实是纯解析模块，其中并没有任何可训练权重。<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>这种分析会提醒我们思考模型的能力源泉来自哪里。</p><p>未完待续。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>打开微信扫码即可见。 <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>记住，一会要考:） <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>当然这里说的“确保”只是计算图层面的，而方案是否可行，数据集的质量和规模往往更重要。 <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>关于 <a href="https://richzhang.github.io/PerceptualSimilarity">LPIPS</a>。 <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>意外不意外？ <a href="#fnref5" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是笔者多年前刚刚接触深度学习时的一些思考，最近才抽空整理出来，
顺便也引用了一些近年新出现的网络结构作为例子。
希望能给同行带来一些有益的启发。&lt;/p&gt;
&lt;p&gt;早些年读深度学习论文的时候经常有一种感觉，作者已经在深入描述网络结构的细节，但笔者自己连其应用环境的大致图景还没有清晰概念。
固然有对其前置工作了解不足的因素，但笔者认为还有一个重要原因是深度学习领域还缺少一种成熟的建模图示标准，类似UML。
一个统一直观的图示系统应该有力地体现出神经网络的一些通用的基本要素，并对建模问题分析有辅助作用。
本文可以视为笔者针对有向图模型通用图示的一个尝试性的提议。&lt;/p&gt;
&lt;p&gt;首先我们尝试从一个简单模型结构入手。&lt;/p&gt;
&lt;h2 id=&quot;简单分类模型&quot;&gt;简单分类模型&lt;/h2&gt;
&lt;p&gt;考虑一个拍照识物应用&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn1&quot; id=&quot;fnref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;，我们可以将其建模为一个简单的分类模型，
用数学语言描述，我们的目标是想获得这样一个函数：&lt;/p&gt;
&lt;p&gt;$$
y’=f_&#92;theta(x)
$$&lt;/p&gt;
&lt;p&gt;其中x代表输入的图像，y’是模型输出的类别概率向量。
函数f是我们建模的一套计算图（譬如CNN网络），其下标θ代表模型权重，可以理解为计算图中待填入的知识。
下文中笔者将计算图函数均写作二元函数，即接受权重和输入数据两个参数。&lt;/p&gt;
&lt;p&gt;模型的训练是一个数值优化问题，通常记作如下表达式的求解：&lt;/p&gt;
&lt;p&gt;$$
&#92;arg &#92;min_&#92;theta { &#92;mathbb{E}_{(x,y)&#92;sim (X,Y)} [l(f(&#92;theta, x), y)] }
$$&lt;/p&gt;
&lt;p&gt;其中(X, Y)是一个有标注数据集，即特征-标签（图像-类别）集合。
小写的x,y是来自数据集中的 (特征, 标签) 采样。
$l$是损失函数，是某种度量&lt;em&gt;模型输出值与标签的距离&lt;/em&gt;的函数，针对不同模态的数据可以有不同的选择。
argmin代表求解某个θ的取值，使得右侧的期望表达式达成最小化。&lt;/p&gt;
&lt;p&gt;数学表达式晦涩难读，但却是理解很多论文的最佳途径，因为其精确给出了问题的全景。
有没有更直观的表达方式？针对上式笔者脑补了如下的图形：&lt;/p&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/diagram-simple-classification.drawio.svg&quot; /&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		简单分类模型图示
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;其中竖着写的~是数学中的服从分布符号，可以理解为从数据集（或某种先验分布）中采集一个样本。&lt;/p&gt;
&lt;p&gt;f上的*代表其中具有&lt;strong&gt;可训练参数&lt;/strong&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn2&quot; id=&quot;fnref2&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt;，即权重θ。
这里其实笔者可以使用颜色来标注，但为了与平时在草纸上手写时的记号习惯保持一致，仍保留了星号。&lt;/p&gt;
&lt;p&gt;圆圈中的ce即损失函数$l$，这里采用分类任务常用的交叉熵(Cross Entropy)。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;loss&lt;/em&gt;左侧的尖朝下三角形▽代表&lt;strong&gt;优化目标&lt;/strong&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn2&quot; id=&quot;fnref2:1&quot;&gt;[2:1]&lt;/a&gt;&lt;/sup&gt;为最小化loss的值。&lt;/p&gt;
&lt;p&gt;整个计算图的方向为由上至下，与很多paper作者采纳的习惯相反。
这是出于以下考虑：&lt;strong&gt;数据&lt;/strong&gt;是绝大多数深度学习任务的基础，因此分析建模问题时我们可以先列出可资利用的各种数据资源。
（按自然的方式）在草稿上把所有数据项列在最上一行，然后从上至下画出可能的模型结构。&lt;/p&gt;
&lt;p&gt;关于模块的形状，笔者考虑把数值模块和纯解析模块区分开来，纯解析模块用圆圈表示，提示其中不含权重（不论是当前训练中的还是来自预训练）。
在笔者想象中，纯解析计算像是一种柏拉图实在（不似尘世之物），而数据集和权重则来自物理实在，充满各种细节和随机性。&lt;/p&gt;
&lt;p&gt;如果是由我们自己来搭建一个针对某个任务的模型计算图，我们只要掌握其中的要点就可以确保做出一个实际可行的方案。&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn3&quot; id=&quot;fnref3&quot;&gt;[3]&lt;/a&gt;&lt;/sup&gt;
为了看出这其中的要点在哪里（同时也为了不失一般性），下面笔者简化了刚才的数学表达式。
由于X, Y都是数据，可以合并为一个符号X来表示；而$f$, $l$都是计算图中的函数，可以合并为函数$g$，于是上式改写成：&lt;/p&gt;
&lt;p&gt;$$
&#92;arg &#92;min_&#92;theta { &#92;mathbb{E}_{x&#92;sim X} [ g(&#92;theta, x) ] }
$$&lt;/p&gt;
&lt;p&gt;我们把右侧的期望表达式记为优化目标A：&lt;/p&gt;
&lt;p&gt;$$
A = &#92;mathbb{E}_{x&#92;sim X} [ g(&#92;theta, x) ]
$$&lt;/p&gt;
&lt;p&gt;刚才说了，模型训练是一个数值优化过程。
具体说是把计算图中每个环节对训练权重θ求导，然后反向传播梯度，这个过程可以（略为粗糙地）记作下式：&lt;/p&gt;
&lt;p&gt;$$
&#92;theta &#92;gets  &#92;theta - &#92;gamma  &#92;cdot &#92;frac{&#92;mathrm{d} }{&#92;mathrm{d} &#92;theta} A
$$&lt;/p&gt;
&lt;p&gt;其中$&#92;gamma$是学习率，对我们的讨论不重要。&lt;/p&gt;
&lt;p&gt;这里$g$就是我们想要搭建的计算图，它的组成部分我们暂时可以当作黑箱。
现在我们可以看出，&lt;strong&gt;约束计算图g的三个要素是数据集X、训练权重θ和优化目标A&lt;/strong&gt;。
这是因为X和θ都是$g$的参数，而A决定了θ怎样得到更新。&lt;/p&gt;
&lt;p&gt;这里还有一个重要的观察是，虽然计算图是从上至下的，但优化过程里只有θ是变量，所以信息的流动方向其实是从X流向θ。
因此，有向图模型的范式可以概括为：凭借&lt;strong&gt;目标A&lt;/strong&gt;，信息由&lt;strong&gt;数据X&lt;/strong&gt;注入&lt;strong&gt;权重θ&lt;/strong&gt;，以此获得智能。
这里的“智能”指的是某种信息编码/解码能力，类似于人类处理问题的快速反应能力，或者叫直觉。&lt;/p&gt;
&lt;p&gt;以下我们使用三要素分析方法，来考察一下各种常见的有向图模型结构。
这里我们仍把计算图中各模块的细节当作黑箱，只关注大框架上各种模型与数据之间的作用方式。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="deep_learning" scheme="https://k-l-lambda.github.io/tags/deep-learning/"/>
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>打造音乐创作AI（一）</title>
    <link href="https://k-l-lambda.github.io/2023/11/29/vae-based-music-encoder/"/>
    <id>https://k-l-lambda.github.io/2023/11/29/vae-based-music-encoder/</id>
    <published>2023-11-29T22:38:03.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><iframe width="900" height="240" src="/klstudio/embed.html#/lotus#/images/mix-score1123_1176-m16-67.json?controls=1"></iframe></picture><figcaption>基于五线谱的AI音乐作品示例</figcaption></figure><p>以本篇作为起始，计划写一个算法作曲相关的系列，记录一些音乐算法研发中的想法。</p><h1>起源</h1><p>2018年Google发布的<a href="https://magenta.tensorflow.org/music-transformer">Music Transformer</a>是算法作曲中具有里程碑意义的工作。它证明了自回归模型在音乐领域中的巨大潜力。其中一个值得重视的基础性工作在其<a href="https://arxiv.org/abs/1809.04281">paper</a>附录中，关于如何把MIDI格式编码成易于自回归模型训练的token序列。</p><p>后来在笔者的<a href="https://en.wikipedia.org/wiki/Optical_music_recognition">OMR</a>项目上线完成后（OMR项目的心得笔者将另开系列分享），笔者发现借助OMR技术的能力，完全可以另辟蹊径，尝试开发一套基于五线谱的音乐生成模型。相对MIDI来说，由于直接来自于作曲家，五线谱的符号系统更接近自然语言。从信息科学角度来说，其编码形式的信息熵密度更大，这是有利于机器学习的。当然缺点也很明显，作为音乐格式，五线谱不像MIDI可以直接播放成声音，阅读门槛较高。不过这个可以靠开发曲谱演奏模型来弥补。而反过来，由于五线谱易于表达音乐构思和指导演奏，把音频和MIDI转成曲谱（即扒谱）也是一个很有价值的方向。无论如何，开发一个可以写作曲谱的AI agent的想法总是很诱人的。</p><span id="more"></span><h1>设计一门语言</h1><p>开发一个曲谱的生成模型，首选要解决的是选定一种用来表达曲谱的语言。就像Music Transformer做的一样，要使用自回归模型生成一种数据格式，就要先处理这种格式的tokenization。而可用的token方案需要满足两个条件：</p><ol><li><strong>token上有明确定义的序</strong>，即各token的先后顺序的规则是明确可学习的。</li><li><strong>语法上是鲁棒的</strong>。</li></ol><p>对于自然语言和MIDI来说第一条都是天然满足的，但对五线谱来说则是个问题。五线谱中每个音乐事件同时存在（时间，声部，音高）三个坐标维度。并且由于存在空间拥挤性，时间上同时发生的多个事件在图像中也没有跟x轴简单对应起来，例如下图：</p><figure><picture><img src="/images/misleading-staff-example.jpg" width="480px" /></picture><figcaption>复杂五线谱举例。蓝框中的音符起始时间相同。</figcaption></figure><p>顺理成章地，我们会想到五线谱的标准数字格式语言是一个选择。首先要排除掉<a href="https://en.wikipedia.org/wiki/MusicXML">Music XML</a>和<a href="https://music-encoding.org/">MEI</a>这类基于XML的格式，太过繁复，破坏了信息熵原则。诸如<a href="http://lilypond.org/">Lilypond</a>和<a href="https://abcnotation.com/">ABC Notation</a>这样的音乐<a href="https://en.wikipedia.org/wiki/Domain-specific_language">DSL</a>则可以在考虑之列。但这时我们就碰到了第二个问题，DSL通常很复杂，语法是不鲁棒的。自回归采样的概率性质不可避免会引入语法错误。由于MIDI格式足够简单，对应的token方案可以通过简单规则忽略掉异常token，而曲谱DSL则做不到（MIDI的tokenization方案可以采用累加性时间token，而五线谱的时间表达以节拍为基准，是一套复杂的分数系统）。这就要求我们重新设计一种便利语言生成模型训练的新曲谱语言。它应具有简单的上下文无关文法，这样的自回归生成采样时就可以采用简单的技术手段来规避语法错误。</p><p>这门新的语言笔者将之命名为“Paraff”，其名字来源于Lilypond中的<a href="https://lilypond.org/doc/v2.23/Documentation/notation/multiple-voices#writing-music-in-parallel">parallel</a>记法。下面是Paraff的“Hello World”：</p><figure><picture><img src="/images/paraff-whole-c.svg" /></picture><figcaption>简单五线谱示例，对应Paraff代码：<em>BOM K0 TN4 TD4 S1 Cg c D1 EOM</em><br/>对比MusicXML的这个<a target="_blank" href="https://en.wikipedia.org/wiki/MusicXML#Example">例子</a>，你会发现Paraff有多么简洁</figcaption></figure><p>Paraff的相关文档，包括词汇表、语法解释器和格式转换工具等，之后整理好笔者会发布在<a href="https://github.com/findlab-org/paraff">Github</a>上。</p><h1>自动编码器</h1><p>Music Transformer作者的一个重要贡献是优化了Transformer二次方复杂度的问题，从而提升了模型处理的序列长度，使得生成的音乐具有超过1分钟的长时间结构。而基于五线谱的生成模型则适于从另一个思路来处理序列长度问题。与MIDI不同的是，曲谱天然具有良好的单元结构，即小节（measure）。因此音乐生成可以自然地建模为两级结构：第一级以Paraff的单词作为token，一小节曲谱为一句子，处理短期注意力；第二级以小节的embedding为序列元素，一首乐曲为一句子，处理长期注意力。(当然在第二级，embedding不是token，并不能单独用于自回归，所以两级是结合在一起训练的。)</p><p>下文重点讨论五线谱中一个小节的embedding如何获取，其他问题留待续篇。</p><p>设想现在我们已经有了大量由Paraff语言表达的单小节曲谱样本，我们目标是把每个样本含有的信息抽象成一个d维（譬如d=256）向量，最自然想到的当然是<a href="https://en.wikipedia.org/wiki/Variational_autoencoder">VAE</a>。本文不详述VAE（变分自动编码器）的基本原理（推荐去读<em>科学空间</em>博主的<a href="https://spaces.ac.cn/search/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/">相关系列</a>），笔者假定读者已了解相关背景知识，这里仅以下图回顾经典VAE的结构：</p><figure><picture><img src="/images/reparameterized-vae.png" width="480px" /></picture><figcaption>VAE经典结构</figcaption></figure><p>如果以自回归模型作为VAE的decoder，则decoder在推断阶段是多步运行的，并且decoder的输入并不只是z，还伴随一个x’的&quot;半成品&quot;。我们观察到无论encoder还是decoder，其主要任务都是分析理解一个序列，区别只在于encoder目标为压缩信息，而decoder目标则是预测下一个词的概率分布。</p><p>基于此，笔者提出一种 encoder和decoder共享大部分权重的新网络结构，即shared VAE。其中encoder和decoder共享一个transformer的主干网络。如下图：</p><figure><picture><img src="/images/shared-vae.drawio.svg" /></picture><figcaption>基于Transformer的shared VAE结构。以上文例举的“Hello World”样本作为x的序列示例。</figcaption></figure><p>Encoder的输出经过reparameter之后，附加到一个特殊token <code>MSUM</code> （意为Measure Summary）的embedding之上，decoder可据此来还原完整的句子。从被训练的主干transformer视角来看，它的任务是，如果看到一个中间词（含<code>BOM</code>）则预测下一个词，如果看到结束符<code>EOM</code>(end of measure)则给出全句概括。也就是一人可以身兼encoder, decoder两份工作。</p><p>最后为了验证这样训练出来的编码器是否能够精确反映原始样本的信息，笔者做了一个试验，将reparameter中的σ数值手工设为常量，观察重构样本的变化，结果如下：</p><table><thead><tr><th style="text-align:left">x</th><th style="text-align:left"><img src="/images/paraff-vae-experiment/score-0.svg" alt="0.svg"></th><th style="text-align:left"><img src="/images/paraff-vae-experiment/score-5.svg" alt="5.svg"></th><th style="text-align:left"><img src="/images/paraff-vae-experiment/score-8.svg" alt="8.svg"></th><th style="text-align:left"><img src="/images/paraff-vae-experiment/score-15.svg" alt="15.svg"></th></tr></thead><tbody><tr><td style="text-align:left">x’<sub>σ=0</sub></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-0-sigma0.svg" alt="0.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-5-sigma0.svg" alt="5.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-8-sigma0.svg" alt="8.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-15-sigma0.svg" alt="15.svg"></td></tr><tr><td style="text-align:left">x’<sub>σ=4</sub></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-0-sigma4.svg" alt="0.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-5-sigma4.svg" alt="5.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-8-sigma4.svg" alt="8.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-15-sigma4.svg" alt="15.svg"></td></tr><tr><td style="text-align:left">x’<sub>σ=8</sub></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-0-sigma8.svg" alt="0.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-5-sigma8.svg" alt="5.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-8-sigma8.svg" alt="8.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-15-sigma8.svg" alt="15.svg"></td></tr><tr><td style="text-align:left">x’<sub>σ=16</sub></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-0-sigma16.svg" alt="0.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-5-sigma16.svg" alt="5.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-8-sigma16.svg" alt="8.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-15-sigma16.svg" alt="15.svg"></td></tr><tr><td style="text-align:left">x’<sub>σ=32</sub></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-0-sigma32.svg" alt="0.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-5-sigma32.svg" alt="5.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-8-sigma32.svg" alt="8.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-15-sigma32.svg" alt="15.svg"></td></tr><tr><td style="text-align:left">x’<sub>σ=100</sub></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-0-sigma100.svg" alt="0.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-5-sigma100.svg" alt="5.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-8-sigma100.svg" alt="8.svg"></td><td style="text-align:left"><img src="/images/paraff-vae-experiment/score-15-sigma100.svg" alt="15.svg"></td></tr></tbody></table><p>试验中选取的原始样本x全部来自无条件随机生成，不包含于shared VAE训练集。</p><p>分析之前先回顾一下reparameterization公式：$$z=\mu + \sigma \epsilon$$其中z, μ, ε都是d维向量，$\epsilon \sim {\mathcal {N}}(0,1)$。</p><p>从试验结果可见，当σ&lt;8时，重构样本与原始样本几乎没有可观察的差异。这表明通过encoder获得的编码不仅精确反映了原始样本的信息，并且对噪声干扰还有很强的鲁棒性。</p><p>这里还有一个问题值得讨论一下。试验观察到encoder输出的μ向量模长在1附近（VAE先验loss的要求μ尽量小），那么一个特征向量是如何对抗<em>标准差8倍于自身模长的高斯噪声</em>的污染，还能顺利给decoder传达信息呢？笔者认为唯一的可能性在于，向量维数d=256的选择带有很大的冗余（当然前提是主干网络得到了充分训练）,decoder得以能够从少数污染较轻的向量分量中还原出原始的μ值。这类似于QRCode的容错原理——部分遮挡的二维码仍能被识别出来，道理相同。</p><p>换个角度，从几何直观来讲，假如绝大多数曲谱对应的μ分布在d维空间中一个低于d维的超平面M上，高维空间中随机选取的噪声向量ε大概率与超平面M垂直（几乎）。这样decoder有一个简单策略，把z投影到M上就得到了μ的近似。当然实际上μ的分布很可能是个形状复杂的流形，但原理上大致类似。</p><p>至此，我们实现了单个小节五线谱的编码器。其应用于音乐生成的具体细节留待后续文章。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
	&lt;picture&gt;
		&lt;iframe width=&quot;900&quot; height=&quot;240&quot; src=&quot;/klstudio/embed.html#/lotus#/images/mix-score1123_1176-m16-67.json?controls=1&quot;&gt;&lt;/iframe&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		基于五线谱的AI音乐作品示例
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;以本篇作为起始，计划写一个算法作曲相关的系列，记录一些音乐算法研发中的想法。&lt;/p&gt;
&lt;h1&gt;起源&lt;/h1&gt;
&lt;p&gt;2018年Google发布的&lt;a href=&quot;https://magenta.tensorflow.org/music-transformer&quot;&gt;Music Transformer&lt;/a&gt;是算法作曲中具有里程碑意义的工作。
它证明了自回归模型在音乐领域中的巨大潜力。
其中一个值得重视的基础性工作在其&lt;a href=&quot;https://arxiv.org/abs/1809.04281&quot;&gt;paper&lt;/a&gt;附录中，关于如何把MIDI格式编码成易于自回归模型训练的token序列。&lt;/p&gt;
&lt;p&gt;后来在笔者的&lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_music_recognition&quot;&gt;OMR&lt;/a&gt;项目上线完成后（OMR项目的心得笔者将另开系列分享），
笔者发现借助OMR技术的能力，完全可以另辟蹊径，尝试开发一套基于五线谱的音乐生成模型。
相对MIDI来说，由于直接来自于作曲家，五线谱的符号系统更接近自然语言。
从信息科学角度来说，其编码形式的信息熵密度更大，这是有利于机器学习的。
当然缺点也很明显，作为音乐格式，五线谱不像MIDI可以直接播放成声音，阅读门槛较高。
不过这个可以靠开发曲谱演奏模型来弥补。
而反过来，由于五线谱易于表达音乐构思和指导演奏，把音频和MIDI转成曲谱（即扒谱）也是一个很有价值的方向。
无论如何，开发一个可以写作曲谱的AI agent的想法总是很诱人的。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="deep_learning" scheme="https://k-l-lambda.github.io/tags/deep-learning/"/>
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="music_algorithm" scheme="https://k-l-lambda.github.io/tags/music-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Solar Centered Logarithmic Map</title>
    <link href="https://k-l-lambda.github.io/2023/10/02/solar-log-map/"/>
    <id>https://k-l-lambda.github.io/2023/10/02/solar-log-map/</id>
    <published>2023-10-02T22:39:48.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><a href="/images/solar-log-scale-horizental.png"><img src="/images/solar-log-scale-thumb.jpg" alt="solar centered logarithmic map thumb" /></a></picture><figcaption>Click to see the original image.</figcaption></figure><span id="more"></span><p>Inner solar system is more dense than outer part, so logarithmic map is popular for universe illustration.But there is a common mistake in many logarithmic maps, like this one:</p><figure><picture><img src="/images/solar-log-wrong.jpg" alt="solar-log-wrong" /></picture><figcaption>an example of logarithmic map with common mistake</figcaption></figure><p>Think about this question: where is the right place for a sun probe whose distance from the sun less than 0.1 AU (say, 0.02 AU) in the map?</p><p>The key problem is the fact: log(0) = -∞，so maybe we simply cannot plot the sun in the map?Yes, certainly we can.And even more, by logarithm we can plot the entire universe into a finite map.After all, all quantities in physics are finite.</p><p>Let’s begin from the planck length.</p><figure><picture><svg xmlns="http://www.w3.org/2000/svg" width="665" height="12600" viewBox="0 0 665 12600" xmlns:xlink="http://www.w3.org/1999/xlink"aria-hidden="true" style="max-width: 100%" preserveAspectRatio="xMinYMin meet"><g transform="rotate(90 332.5 332.5)"><image href="/images/solar-log-scale-horizental.png" x="0" y="0" width="12600" height="665" /></g></svg></picture><figcaption></figcaption></figure><p><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> <sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> <sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Considering the <a href="https://en.wikipedia.org/wiki/T-duality">T-duality</a>, space in scale less than 1 planck length may be equal to that greater than 1 planck length (R &lt;=&gt; 1/R). So God may be homeless in that case. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>The vast blank area hints that the present theory may omit some significant existences. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Notice that woman is on the right, so she is taller in fact. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>I bet you live here now. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Here I adopt the theory of <a href="https://en.wikipedia.org/wiki/Eternal_inflation">Eternal inflation</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
	&lt;picture&gt;
		&lt;a href=&quot;/images/solar-log-scale-horizental.png&quot;&gt;
			&lt;img src=&quot;/images/solar-log-scale-thumb.jpg&quot; alt=&quot;solar centered logarithmic map thumb&quot; /&gt;
		&lt;/a&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		Click to see the original image.
	&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
    
      <category term="comic" scheme="https://k-l-lambda.github.io/tags/comic/"/>
    
  </entry>
  
  <entry>
    <title>Contemporary Art Genres and Artists Style Exhibition</title>
    <link href="https://k-l-lambda.github.io/2023/05/28/ai-art-styles/"/>
    <id>https://k-l-lambda.github.io/2023/05/28/ai-art-styles/</id>
    <published>2023-05-28T23:07:46.000Z</published>
    <updated>2025-04-23T16:01:47.648Z</updated>
    
    <content type="html"><![CDATA[<iframe src="/klstudio/art-styles/genres.html" style="border: 0; width: 100%; height: 676px"></iframe><span id="more"></span><iframe src="/klstudio/art-styles/genres2.html" style="border: 0; width: 100%; height: 960px"></iframe><iframe src="/klstudio/art-styles/artists.html" style="border: 0; width: 100%; height: 9066px"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;iframe src=&quot;/klstudio/art-styles/genres.html&quot; style=&quot;border: 0; width: 100%; height: 676px&quot;&gt;&lt;/iframe&gt;
    
    </summary>
    
    
    
      <category term="deep_learning" scheme="https://k-l-lambda.github.io/tags/deep-learning/"/>
    
      <category term="aigc" scheme="https://k-l-lambda.github.io/tags/aigc/"/>
    
      <category term="art" scheme="https://k-l-lambda.github.io/tags/art/"/>
    
  </entry>
  
  <entry>
    <title>[xkcd] 外星举手击掌</title>
    <link href="https://k-l-lambda.github.io/2023/05/28/xkcd-ExoplanetHigh5/"/>
    <id>https://k-l-lambda.github.io/2023/05/28/xkcd-ExoplanetHigh5/</id>
    <published>2023-05-28T14:54:02.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><img src="/images/xkcd-exoplanet_high-5.png" class="figure" width="515" alt="xkcd 2779"></picture><figcaption>原作：<a href="https://xkcd.com/2779/">xkcd/2779</a></figcaption></figure><figure><picture><img src="/images/terminator2-high5.webp" class="figure" width="400"></picture></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/xkcd-exoplanet_high-5.png&quot; class=&quot;figure&quot; width=&quot;515&quot; alt=&quot;xkcd 2779&quot;&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;原作：&lt;
      
    
    </summary>
    
    
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="comic" scheme="https://k-l-lambda.github.io/tags/comic/"/>
    
      <category term="translation" scheme="https://k-l-lambda.github.io/tags/translation/"/>
    
      <category term="xkcd" scheme="https://k-l-lambda.github.io/tags/xkcd/"/>
    
  </entry>
  
  <entry>
    <title>Illustration for Diffusion-Based Generative Model</title>
    <link href="https://k-l-lambda.github.io/2023/04/22/diffusion-model-illustration/"/>
    <id>https://k-l-lambda.github.io/2023/04/22/diffusion-model-illustration/</id>
    <published>2023-04-22T14:49:24.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><img src="/images/diffusion-training.svg" width="480" /></picture><picture><img src="/images/diffusion-sampling.svg" width="480" /></picture><figcaption>The Illustration for Training Procedure & Sampling Procedure</figcaption></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/diffusion-training.svg&quot; width=&quot;480&quot; /&gt;
	&lt;/picture&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/diffusion-sampling
      
    
    </summary>
    
    
    
      <category term="deep_learning" scheme="https://k-l-lambda.github.io/tags/deep-learning/"/>
    
      <category term="diffusion_model" scheme="https://k-l-lambda.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>[xkcd] 宇宙价格等级</title>
    <link href="https://k-l-lambda.github.io/2022/09/01/xkcd-UniversePriceTiers/"/>
    <id>https://k-l-lambda.github.io/2022/09/01/xkcd-UniversePriceTiers/</id>
    <published>2022-09-01T23:18:20.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<p>原作：<a href="https://xkcd.com/2666/">xkcd/2666</a></p><figure><picture><img src="/images/xkcd-universe_price_tiers-zh.png" class="figure" width="646" alt="xkcd 2657"></picture><figcaption>宇宙价格等级</figcaption></figure><p>&quot;Sow the wind, reap the whirlwind&quot;是一句西方谚语，相当于“种瓜得瓜，种豆得豆”，或“善有善报，恶有恶报”。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原作：&lt;a href=&quot;https://xkcd.com/2666/&quot;&gt;xkcd/2666&lt;/a&gt;&lt;/p&gt;
&lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/xkcd-universe_price_tiers-zh.png&quot; class=&quot;fi
      
    
    </summary>
    
    
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="comic" scheme="https://k-l-lambda.github.io/tags/comic/"/>
    
      <category term="translation" scheme="https://k-l-lambda.github.io/tags/translation/"/>
    
      <category term="xkcd" scheme="https://k-l-lambda.github.io/tags/xkcd/"/>
    
  </entry>
  
  <entry>
    <title>[xkcd] “复”元音</title>
    <link href="https://k-l-lambda.github.io/2022/08/13/xkcd-ComplexVowels/"/>
    <id>https://k-l-lambda.github.io/2022/08/13/xkcd-ComplexVowels/</id>
    <published>2022-08-13T15:55:33.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><img src="/images/xkcd-complex_vowels.png" class="figure" width="270" alt="xkcd 2657"></picture><figcaption>原作：<a href="https://xkcd.com/2657/">xkcd/2657</a></figcaption></figure><blockquote><p>语言学提示：<br/>沿虚轴扩展国际音标元音平面，由此生成&quot;<strong>复</strong>元音&quot;<br/>——一种凡人无法理解的晦气声音。</p></blockquote><p>国际音标中的元音可以按发音位置(前-后)和口型(闭-开)两个维度分类, 即所谓’<a href="https://en.wikipedia.org/wiki/Vowel_diagram">VOWEL PLANE</a>’.如下图:</p><img src="/images/English_vowel_chart.svg.png" class="figure" width="240" alt="vowel plane"><p>作者借用了数学上的<a href="https://zh.m.wikipedia.org/zh-hans/%E8%A7%A3%E6%9E%90%E5%BB%B6%E6%8B%93">解析延拓</a>概念,设想在元音平面基础上再加上虚轴, 即得到了&quot;复元音&quot;.这里的&quot;复&quot;是复数的复. (双元音的英文是’diphthong’.)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/xkcd-complex_vowels.png&quot; class=&quot;figure&quot; width=&quot;270&quot; alt=&quot;xkcd 2657&quot;&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;原作：&lt;a 
      
    
    </summary>
    
    
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="comic" scheme="https://k-l-lambda.github.io/tags/comic/"/>
    
      <category term="translation" scheme="https://k-l-lambda.github.io/tags/translation/"/>
    
      <category term="xkcd" scheme="https://k-l-lambda.github.io/tags/xkcd/"/>
    
  </entry>
  
  <entry>
    <title>[xkcd] 视角直径拐点</title>
    <link href="https://k-l-lambda.github.io/2022/08/13/xkcd-AngularDiameterTurnaround/"/>
    <id>https://k-l-lambda.github.io/2022/08/13/xkcd-AngularDiameterTurnaround/</id>
    <published>2022-08-13T14:46:11.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><img src="/images/xkcd-angular_diameter_turnaround.png" class="figure" width="740" alt="xkcd 2622"></picture><figcaption>原作：<a href="https://xkcd.com/2622/">xkcd/2622</a></figcaption></figure><blockquote><p><strong>视角直径拐点</strong><br/><em>(图中用手机代替星系，亮度和红移已调整到可见范围)</em><br/><em>近大远小，离我们越远的物体看起来越小，但非常非常远的物体看起来反而会变大！</em><br/><em>因为当它们的光线刚刚发射出来的时候宇宙还很小，当时它们距离我们（现在的空间位置）更近。</em><br/></p></blockquote><p>由于光速有限, 眺望远方其实也是眺望过去(字面意义上的).所以整个夜空其实是展示给我们的一部宇宙历史, 有意思的是这历史的最外层其实是宇宙早期的显微结构(参见<a href="https://wikizh.click/wiki/Decoupling_(cosmology)">解耦</a>).设想, 从地球向外, 把夜空(过去光锥)划分成一圈圈包围我们的天球球层, 在地球附近这些球层半径不断变大, 最后又变得很小, 那么中间一定会经历一个拐点.我有点好奇这拐点具体发生在什么时间, 从这篇xkcd的漫画图中来看, 大概是发生在<a href="https://zh.m.wikipedia.org/zh-hans/%E5%86%8D%E9%9B%BB%E9%9B%A2">再电离</a>事件之前的.</p><hr><p>好久没有更新博客了, 去年至今其实有一些酝酿中的想法, 没时间整理成paper, 只能再往后推了.最近打算先翻译一些有趣的xkcd漫画作品.</p><p>非常怀念过去在科学松鼠会上看<a href="http://www.songshuehu.net/ent.htm">Ent</a><sup><a href="https://www.163.com/dy/article/GBP09SFP055218MP.html">*</a></sup>翻译xkcd等各种科学漫画的时光.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/xkcd-angular_diameter_turnaround.png&quot; class=&quot;figure&quot; width=&quot;740&quot; alt=&quot;xkcd 2622&quot;&gt;
	&lt;/picture&gt;
	&lt;figc
      
    
    </summary>
    
    
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="comic" scheme="https://k-l-lambda.github.io/tags/comic/"/>
    
      <category term="translation" scheme="https://k-l-lambda.github.io/tags/translation/"/>
    
      <category term="xkcd" scheme="https://k-l-lambda.github.io/tags/xkcd/"/>
    
  </entry>
  
  <entry>
    <title>[转载] 梁文道：聊聊知识是什么</title>
    <link href="https://k-l-lambda.github.io/2021/06/24/what-is-knowledge/"/>
    <id>https://k-l-lambda.github.io/2021/06/24/what-is-knowledge/</id>
    <published>2021-06-24T21:50:26.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<p><audio src="/images/liang-what-is-knowledge.mp3" style="width: 100%; max-width: 800px" controls></audio></p><p>来自：<a href="https://www.youtube.com/watch?v=8BgV5Z8rT8Y&amp;list=FLWjEpsPk3N_BX9YPHNtB5Ow&amp;index=1"><em>YouTube</em> 梁文道：聊聊知识是什么</a><br/>上一集：<a href="https://www.youtube.com/watch?v=nH3YrhDEDdQ&amp;list=FLWjEpsPk3N_BX9YPHNtB5Ow&amp;index=2"><em>YouTube</em> 梁文道：聊聊反智主义是如何兴起的</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;audio src=&quot;/images/liang-what-is-knowledge.mp3&quot; style=&quot;width: 100%; max-width: 800px&quot; controls&gt;&lt;/audio&gt;&lt;/p&gt;
&lt;p&gt;来自：&lt;a href=&quot;https://www.y
      
    
    </summary>
    
    
    
      <category term="中文" scheme="https://k-l-lambda.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="belief" scheme="https://k-l-lambda.github.io/tags/belief/"/>
    
  </entry>
  
  <entry>
    <title>How to represent a Rubik&#39;s cube state in a calculable form?</title>
    <link href="https://k-l-lambda.github.io/2020/12/14/rubik-cube-notation/"/>
    <id>https://k-l-lambda.github.io/2020/12/14/rubik-cube-notation/</id>
    <published>2020-12-14T21:37:33.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><img src="/images/cube3-matrix-tetris.svg" alt="a cube3 matrix samle #LKONLKONLOKNLKONKLNOGCAAFD" /></picture><figcaption>Guess what this is? <a target="_blank" href="/klstudio/#/documents/dynamic-labeled-cube3#LKONLKONLOKNLKONKLNOGCAAFD">See the answer here.</a></figcaption></figure><p>To describe a chess game state, you don’t need to mention every chess piece’s details.Essentially, a chess piece is just a symbol. Ignoring appearance detail helps us to grab the game gist.Till now, a popular way to represent a Rubik’s cube state is the facelets expanding graph, like this:</p><p><img src="/images/cube3-expand-graph-original.png" alt="a cube3 expanding graph sample"></p><p>By this way, you can’t tell which facelets are adjacent each other straightway, also it’s hard to imagine what the cube will change into after a twist applied.That because it comes from the appearance, but not the essential.Furtherly, as <a href="/2020/02/05/cube-algebra/#Motivation">my preivous blog</a> wrote, Rubik’s Cube solver programs who construct cube state from facelet color is clumsy.Rubik’s Cube is a game about cubes’s <strong>rotation</strong> and <strong>permutation</strong> (but not painting color), <strong>matrix</strong> is the most proper math tool here.</p><p>However, the fact revealed by this method is not easy to see through, and that is just what I will tell you in this blog.</p><span id="more"></span><h2 id="Cube-orientation-representation">Cube orientation representation</h2><p>Let’s begin with a simple case, the <em>1-order Rubik’s cube</em>, i.e. a single cube.How to restore a rotated cube to the original orientation, in a shorttest way?That’s just <a href="/2020/02/05/cube-algebra/">Cube Rotation Algebra</a>’s topic.Now let’s symbolize them in a new way.</p><p>Recently, I found 2 lucky things, this is the first:</p><figure><span class="max600"><span class="fixed-ratio" style="width: 100%; padding-top: 100%"><iframe src="/klstudio/embed.html#/documents/mesh-viewer-demo:quarter-array-4x6-greek"></iframe></span></span><figcaption>Representing 24 orthogonal orientations by lowercase Greek letters.</figcaption></figure><p>You will be familiar with this figure if you have read <a href="/2020/02/05/cube-algebra/">Cube Rotation Algebra</a>.The lucky is that we have exact 24 modern Greek letters in total coincidentally.</p><p>I have explained cube rotation algebra in <a href="/2020/02/05/cube-algebra/">the preivous blog</a> in details,now I just show you what the relationship looks like between Greek letters and colored cubes.</p><figure><span style="display: inline-block; width: 400px;"><span class="fixed-ratio" style="width: 100%; padding-top: 100%"><iframe src="/klstudio/embed.html#/documents/flipping-cube-demo"></iframe></span></span></figure><p>Mathematically, they can be defined in quaternions:</p><p>$$\begin{aligned}&amp; \alpha = 1 \\&amp; \beta = i \\&amp; \gamma = j \\&amp; \delta = k \\&amp; \epsilon = \sqrt{i} = \frac{\sqrt{2}}{2} + \frac{\sqrt{2}}{2} i \\&amp; … \\&amp; \omega = i\sqrt[-]{k} = \frac{\sqrt{2}}{2} i - \frac{\sqrt{2}}{2} j\end{aligned}$$</p><p>These 24 elements make up a group <a href="https://en.wikipedia.org/wiki/Octahedral_symmetry#Full_octahedral_symmetry">$O_{h}$</a>,that means they are closed for multiplication, and the multiplication satisfys the associative law.</p><p>And this is the group multiplication table:</p><p>$\begin{matrix}\times &amp; \boldsymbol{\alpha} &amp; \boldsymbol{\beta} &amp; \boldsymbol{\gamma} &amp; \boldsymbol{\delta} &amp; \boldsymbol{\epsilon} &amp; \boldsymbol{\zeta} &amp; \boldsymbol{\eta} &amp; \boldsymbol{\theta} &amp; \boldsymbol{\iota} &amp; \boldsymbol{\kappa} &amp; \boldsymbol{\lambda} &amp; \boldsymbol{\mu} &amp; \boldsymbol{\nu} &amp; \boldsymbol{\xi} &amp; \boldsymbol{\omicron} &amp; \boldsymbol{\pi} &amp; \boldsymbol{\rho} &amp; \boldsymbol{\sigma} &amp; \boldsymbol{\tau} &amp; \boldsymbol{\upsilon} &amp; \boldsymbol{\phi} &amp; \boldsymbol{\chi} &amp; \boldsymbol{\psi} &amp; \boldsymbol{\omega} \\\boldsymbol{\alpha} &amp; \alpha &amp; \beta &amp; \gamma &amp; \delta &amp; \epsilon &amp; \zeta &amp; \eta &amp; \theta &amp; \iota &amp; \kappa &amp; \lambda &amp; \mu &amp; \nu &amp; \xi &amp; \omicron &amp; \pi &amp; \rho &amp; \sigma &amp; \tau &amp; \upsilon &amp; \phi &amp; \chi &amp; \psi &amp; \omega \\\boldsymbol{\beta} &amp; \beta &amp; \alpha &amp; \delta &amp; \gamma &amp; \theta &amp; \chi &amp; \omega &amp; \epsilon &amp; \phi &amp; \psi &amp; \pi &amp; \omicron &amp; \sigma &amp; \rho &amp; \mu &amp; \lambda &amp; \xi &amp; \nu &amp; \upsilon &amp; \tau &amp; \iota &amp; \zeta &amp; \kappa &amp; \eta \\\boldsymbol{\gamma} &amp; \gamma &amp; \delta &amp; \alpha &amp; \beta &amp; \tau &amp; \iota &amp; \psi &amp; \upsilon &amp; \zeta &amp; \omega &amp; \mu &amp; \lambda &amp; \rho &amp; \sigma &amp; \pi &amp; \omicron &amp; \nu &amp; \xi &amp; \epsilon &amp; \theta &amp; \chi &amp; \phi &amp; \eta &amp; \kappa \\\boldsymbol{\delta} &amp; \delta &amp; \gamma &amp; \beta &amp; \alpha &amp; \upsilon &amp; \phi &amp; \kappa &amp; \tau &amp; \chi &amp; \eta &amp; \omicron &amp; \pi &amp; \xi &amp; \nu &amp; \lambda &amp; \mu &amp; \sigma &amp; \rho &amp; \theta &amp; \epsilon &amp; \zeta &amp; \iota &amp; \omega &amp; \psi \\\boldsymbol{\epsilon} &amp; \epsilon &amp; \theta &amp; \upsilon &amp; \tau &amp; \beta &amp; \xi &amp; \lambda &amp; \alpha &amp; \nu &amp; \mu &amp; \omega &amp; \psi &amp; \phi &amp; \chi &amp; \kappa &amp; \eta &amp; \zeta &amp; \iota &amp; \gamma &amp; \delta &amp; \sigma &amp; \rho &amp; \omicron &amp; \pi \\\boldsymbol{\zeta} &amp; \zeta &amp; \phi &amp; \iota &amp; \chi &amp; \lambda &amp; \gamma &amp; \rho &amp; \omicron &amp; \alpha &amp; \xi &amp; \tau &amp; \epsilon &amp; \eta &amp; \omega &amp; \upsilon &amp; \theta &amp; \psi &amp; \kappa &amp; \mu &amp; \pi &amp; \delta &amp; \beta &amp; \nu &amp; \sigma \\\boldsymbol{\eta} &amp; \eta &amp; \psi &amp; \omega &amp; \kappa &amp; \nu &amp; \lambda &amp; \delta &amp; \rho &amp; \pi &amp; \alpha &amp; \phi &amp; \iota &amp; \upsilon &amp; \epsilon &amp; \zeta &amp; \chi &amp; \tau &amp; \theta &amp; \sigma &amp; \xi &amp; \omicron &amp; \mu &amp; \gamma &amp; \beta \\\boldsymbol{\theta} &amp; \theta &amp; \epsilon &amp; \tau &amp; \upsilon &amp; \alpha &amp; \rho &amp; \pi &amp; \beta &amp; \sigma &amp; \omicron &amp; \eta &amp; \kappa &amp; \iota &amp; \zeta &amp; \psi &amp; \omega &amp; \chi &amp; \phi &amp; \delta &amp; \gamma &amp; \nu &amp; \xi &amp; \mu &amp; \lambda \\\boldsymbol{\iota} &amp; \iota &amp; \chi &amp; \zeta &amp; \phi &amp; \mu &amp; \alpha &amp; \nu &amp; \pi &amp; \gamma &amp; \sigma &amp; \epsilon &amp; \tau &amp; \psi &amp; \kappa &amp; \theta &amp; \upsilon &amp; \eta &amp; \omega &amp; \lambda &amp; \omicron &amp; \beta &amp; \delta &amp; \rho &amp; \xi \\\boldsymbol{\kappa} &amp; \kappa &amp; \omega &amp; \psi &amp; \eta &amp; \xi &amp; \omicron &amp; \alpha &amp; \sigma &amp; \mu &amp; \delta &amp; \zeta &amp; \chi &amp; \epsilon &amp; \upsilon &amp; \phi &amp; \iota &amp; \theta &amp; \tau &amp; \rho &amp; \nu &amp; \lambda &amp; \pi &amp; \beta &amp; \gamma \\\boldsymbol{\lambda} &amp; \lambda &amp; \omicron &amp; \pi &amp; \mu &amp; \phi &amp; \omega &amp; \tau &amp; \zeta &amp; \eta &amp; \epsilon &amp; \sigma &amp; \nu &amp; \delta &amp; \beta &amp; \xi &amp; \rho &amp; \gamma &amp; \alpha &amp; \iota &amp; \chi &amp; \kappa &amp; \psi &amp; \upsilon &amp; \theta \\\boldsymbol{\mu} &amp; \mu &amp; \pi &amp; \omicron &amp; \lambda &amp; \chi &amp; \kappa &amp; \epsilon &amp; \iota &amp; \psi &amp; \tau &amp; \xi &amp; \rho &amp; \beta &amp; \delta &amp; \sigma &amp; \nu &amp; \alpha &amp; \gamma &amp; \zeta &amp; \phi &amp; \omega &amp; \eta &amp; \theta &amp; \upsilon \\\boldsymbol{\nu} &amp; \nu &amp; \rho &amp; \xi &amp; \sigma &amp; \psi &amp; \epsilon &amp; \phi &amp; \eta &amp; \upsilon &amp; \iota &amp; \beta &amp; \gamma &amp; \omicron &amp; \mu &amp; \alpha &amp; \delta &amp; \lambda &amp; \pi &amp; \omega &amp; \kappa &amp; \theta &amp; \tau &amp; \zeta &amp; \chi \\\boldsymbol{\xi} &amp; \xi &amp; \sigma &amp; \nu &amp; \rho &amp; \omega &amp; \upsilon &amp; \zeta &amp; \kappa &amp; \epsilon &amp; \chi &amp; \gamma &amp; \beta &amp; \lambda &amp; \pi &amp; \delta &amp; \alpha &amp; \omicron &amp; \mu &amp; \psi &amp; \eta &amp; \tau &amp; \theta &amp; \phi &amp; \iota \\\boldsymbol{\omicron} &amp; \omicron &amp; \lambda &amp; \mu &amp; \pi &amp; \zeta &amp; \psi &amp; \theta &amp; \phi &amp; \kappa &amp; \upsilon &amp; \rho &amp; \xi &amp; \alpha &amp; \gamma &amp; \nu &amp; \sigma &amp; \beta &amp; \delta &amp; \chi &amp; \iota &amp; \eta &amp; \omega &amp; \epsilon &amp; \tau \\\boldsymbol{\pi} &amp; \pi &amp; \mu &amp; \lambda &amp; \omicron &amp; \iota &amp; \eta &amp; \upsilon &amp; \chi &amp; \omega &amp; \theta &amp; \nu &amp; \sigma &amp; \gamma &amp; \alpha &amp; \rho &amp; \xi &amp; \delta &amp; \beta &amp; \phi &amp; \zeta &amp; \psi &amp; \kappa &amp; \tau &amp; \epsilon \\\boldsymbol{\rho} &amp; \rho &amp; \nu &amp; \sigma &amp; \xi &amp; \eta &amp; \tau &amp; \chi &amp; \psi &amp; \theta &amp; \zeta &amp; \delta &amp; \alpha &amp; \pi &amp; \lambda &amp; \gamma &amp; \beta &amp; \mu &amp; \omicron &amp; \kappa &amp; \omega &amp; \upsilon &amp; \epsilon &amp; \iota &amp; \phi \\\boldsymbol{\sigma} &amp; \sigma &amp; \xi &amp; \rho &amp; \nu &amp; \kappa &amp; \theta &amp; \iota &amp; \omega &amp; \tau &amp; \phi &amp; \alpha &amp; \delta &amp; \mu &amp; \omicron &amp; \beta &amp; \gamma &amp; \pi &amp; \lambda &amp; \eta &amp; \psi &amp; \epsilon &amp; \upsilon &amp; \chi &amp; \zeta \\\boldsymbol{\tau} &amp; \tau &amp; \upsilon &amp; \theta &amp; \epsilon &amp; \delta &amp; \sigma &amp; \mu &amp; \gamma &amp; \rho &amp; \lambda &amp; \kappa &amp; \eta &amp; \chi &amp; \phi &amp; \omega &amp; \psi &amp; \iota &amp; \zeta &amp; \alpha &amp; \beta &amp; \xi &amp; \nu &amp; \pi &amp; \omicron \\\boldsymbol{\upsilon} &amp; \upsilon &amp; \tau &amp; \epsilon &amp; \theta &amp; \gamma &amp; \nu &amp; \omicron &amp; \delta &amp; \xi &amp; \pi &amp; \psi &amp; \omega &amp; \zeta &amp; \iota &amp; \eta &amp; \kappa &amp; \phi &amp; \chi &amp; \beta &amp; \alpha &amp; \rho &amp; \sigma &amp; \lambda &amp; \mu \\\boldsymbol{\phi} &amp; \phi &amp; \zeta &amp; \chi &amp; \iota &amp; \omicron &amp; \beta &amp; \sigma &amp; \lambda &amp; \delta &amp; \nu &amp; \theta &amp; \upsilon &amp; \kappa &amp; \psi &amp; \epsilon &amp; \tau &amp; \omega &amp; \eta &amp; \pi &amp; \mu &amp; \alpha &amp; \gamma &amp; \xi &amp; \rho \\\boldsymbol{\chi} &amp; \chi &amp; \iota &amp; \phi &amp; \zeta &amp; \pi &amp; \delta &amp; \xi &amp; \mu &amp; \beta &amp; \rho &amp; \upsilon &amp; \theta &amp; \omega &amp; \eta &amp; \tau &amp; \epsilon &amp; \kappa &amp; \psi &amp; \omicron &amp; \lambda &amp; \gamma &amp; \alpha &amp; \sigma &amp; \nu \\\boldsymbol{\psi} &amp; \psi &amp; \eta &amp; \kappa &amp; \omega &amp; \rho &amp; \mu &amp; \beta &amp; \nu &amp; \omicron &amp; \gamma &amp; \chi &amp; \zeta &amp; \theta &amp; \tau &amp; \iota &amp; \phi &amp; \epsilon &amp; \upsilon &amp; \xi &amp; \sigma &amp; \pi &amp; \lambda &amp; \alpha &amp; \delta \\\boldsymbol{\omega} &amp; \omega &amp; \kappa &amp; \eta &amp; \psi &amp; \sigma &amp; \pi &amp; \gamma &amp; \xi &amp; \lambda &amp; \beta &amp; \iota &amp; \phi &amp; \tau &amp; \theta &amp; \chi &amp; \zeta &amp; \upsilon &amp; \epsilon &amp; \nu &amp; \rho &amp; \mu &amp; \omicron &amp; \delta &amp; \alpha\end{matrix}$</p><p>This is the multiplication visualization:</p><figure><iframe src="/klstudio/embed.html#/documents/cube-multiplication-demo" width="600" height="280"></iframe></figure><h2 id="Cubies’-position-representation">Cubies’ position representation</h2><p>In 3-order Rubik’s cube, we have $3^3-1=26$ cubies.And this is the second lucky thing: there are 26 Latin letters in total.So we can labeled 26 cubie positions by A-Z. It looks like this:</p><figure><span class="max600"><span class="fixed-ratio" style="width: 100%; padding-top: 100%"><iframe src="/klstudio/embed.html#/documents/static-labeled-cube3-demo"></iframe></span></span><figcaption>Rubik's cube with labels on cubies</figcaption></figure><p>Mathematically, we define 26 one-hot vectors:</p><p>$$\boldsymbol{A} = (\mathbf{1}, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) \\\boldsymbol{B} = (0, \mathbf{1}, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) \\\boldsymbol{C} = (0, 0, \mathbf{1}, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) \\… \\\boldsymbol{Z} = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \mathbf{1})$$</p><p>This form is convenient for constructing matrices, e.g. a 26×26 identity matrix can be represented as:</p><p>$$\begin{bmatrix}\boldsymbol{A}^T &amp; \boldsymbol{B}^T &amp; \boldsymbol{C}^T &amp; … &amp; \boldsymbol{X}^T &amp; \boldsymbol{Y}^T &amp; \boldsymbol{Z}^T\end{bmatrix}^T$$</p><h2 id="Cube-rotation-and-position-permutation">Cube rotation and position permutation</h2><p>Rotation causes displacement, it’s a kind of linear transformation. We can see this clearly in matrix.Take this simple example firstly:</p><figure><picture><img src="/images/rect2x2-permutation.drawio.svg" /></picture></figure><style>.red{color: red;}</style><p>We have 4 (2×2) boxes labeled by red <em class="red">A</em> <em class="red">B</em> <em class="red">C</em> <em class="red">D</em>,and we have 4 fixed cells labeled by black <em>A</em> <em>B</em> <em>C</em> <em>D</em>.</p><p>Before and after a 90° rotation, we record boxes’ position in a 4×4 table:</p><style>table td{text-align: center;}table.no-border{border: 0;}table.no-border > tbody > tr > td{border: 0;}table strong{color: red;}</style><table class="no-border"><tr><td><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">A</th><th style="text-align:center">B</th><th style="text-align:center">C</th><th style="text-align:center">D</th></tr></thead><tbody><tr><td style="text-align:center"><strong>A</strong></td><td style="text-align:center">1</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><strong>B</strong></td><td style="text-align:center"></td><td style="text-align:center">1</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><strong>C</strong></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">1</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><strong>D</strong></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">1</td></tr></tbody></table></td><td style="font-size: 400%">&#x21e8;</td><td><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">A</th><th style="text-align:center">B</th><th style="text-align:center">C</th><th style="text-align:center">D</th></tr></thead><tbody><tr><td style="text-align:center"><strong>A</strong></td><td style="text-align:center"></td><td style="text-align:center">1</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><strong>B</strong></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">1</td></tr><tr><td style="text-align:center"><strong>C</strong></td><td style="text-align:center">1</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><strong>D</strong></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">1</td><td style="text-align:center"></td></tr></tbody></table></td></table><p>So this is the matrix’s meaning, every row tells us which cell the box is at.</p><p>$$\alpha\begin{bmatrix}1 &amp;  &amp;  &amp; \\&amp; 1 &amp;  &amp; \\&amp;  &amp; 1 &amp; \\&amp;  &amp;  &amp;1\end{bmatrix} \to \eta \begin{bmatrix}&amp; 1 &amp;  &amp; \\&amp;  &amp;  &amp; 1 \\1 &amp;  &amp;  &amp; \\&amp;  &amp; 1 &amp;\end{bmatrix} = \begin{bmatrix}&amp; \eta &amp;  &amp; \\&amp;  &amp;  &amp; \eta \\\eta &amp;  &amp;  &amp; \\&amp;  &amp; \eta &amp;\end{bmatrix}$$</p><p>And plused an orientation scalar in Greek letters.(Why <em>η</em>? Try to look up what <em>η</em> stands for in the 24 knights figure.)</p><p>Now let’s extend 2×2 into 26×26:</p><figure><iframe src="/klstudio/embed.html#/documents/dynamic-labeled-cube3-demo" width="960" height="600"></iframe></figure><p>For the matrix you see above, I arranged the cubies’ order as this:</p><figure><picture><img src="/images/cube3-matrix-illustration.drawio.svg" style="max-width: 600px" /></picture></figure><p>A state matrix can be decomposed into 2 parts: orientations and position permutation.And position permutation is determined by orientations, so to present a Rubik’s cube state we only need to record a vector of orientation symbols.Mathematically, we have a state vector:</p><p>$$S=o_i | _{i=1,…,26}$$</p><p>while</p><p>$$o_i \in \left \{ \alpha, \beta, \gamma, …, \omega \right \}$$</p><p>then the state matrix is:</p><p>$$mat(S)=diag(S) \cdot displace(o_i, P_i)^T|_{i=1,…,26}$$</p><p>$$P_i = \boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}, …, \boldsymbol{Z} |_{\text{when } i=1,…,26}$$</p><p>while <em>diag</em> stands for diagonal matrix, <em>displace</em> is a position mapping by a specific orientation, from a one-hot position vector to another.This is the dispacement table:</p><p>$$\begin{matrix}\text{displace} &amp; \alpha &amp; \beta &amp; \gamma &amp; … \\\color{red} A &amp; \boldsymbol{A} &amp; \boldsymbol{G} &amp; \boldsymbol{F} &amp; … \\\color{red} B &amp; \boldsymbol{B} &amp; \boldsymbol{H} &amp; \boldsymbol{E} &amp; … \\\color{red} C &amp; \boldsymbol{C} &amp; \boldsymbol{E} &amp; \boldsymbol{H} &amp; … \\… &amp; &amp; &amp; &amp;\end{matrix}$$</p><p>The whole table is 26×24, what shows here is a part, and you can imagine the rest.</p><p>Let’s take the top matrix in this blog (<a href="/klstudio/embed.html#/documents/dynamic-labeled-cube3#LKONLKONLOKNLKONKLNOGCAAFD">the tetris pattern</a>) as an example:</p><p>$\begin{aligned}&amp; mat(\omicron, \lambda, \pi, \mu, \omicron, \lambda, \pi, \mu, \lambda, \omicron, \omicron, \lambda, \pi, \mu, \pi, \mu, \omicron, \mu, \lambda, \pi, \eta, \kappa, \iota, \zeta, \alpha, \alpha) \\= &amp; diag(\omicron, \lambda, \pi, \mu, \omicron, \lambda, \pi, \mu, \lambda, \omicron, \omicron, \lambda, \pi, \mu, \pi, \mu, \omicron, \mu, \lambda, \pi, \eta, \kappa, \iota, \zeta, \alpha, \alpha) \cdot \left [ F^T C^T B^T G^T H^T A^T D^T E^T O^T P^T S^T Q^T L^T K^T R^T T^T J^T N^T I^T M^T U^T V^T W^T X^T Y^T Z^T \right ]\end{aligned}$</p><p>For short, I will refer it as:</p><p>$$\left \langle \omicron \lambda \pi \mu \omicron \lambda \pi \mu \lambda \omicron \omicron \lambda \pi \mu \pi \mu \omicron \mu \lambda \pi \eta \kappa \iota \zeta \alpha \alpha \right \rangle$$</p><h3 id="State-space-capacity">State space capacity</h3><p>3D object has 3 degrees of freedom in rotation, but the most convenient method to represent it is using 4 fields, i.e. quaternion.So a proper state space redundancy is necessary for the sake of calculation.</p><p>A valid 3-order Rubik’s cube’s total variation is:</p><p>$$\frac{8! \times 3^8 \times 12! \times 2^{12}}{2 \times 2 \times 3} = 43252003274489856000 \approx 4.33 \times 10^{19}$$</p><p>If allowing disassembly, the number becomes twelve times larger:</p><p>$$8! \times 3^8 \times 12! \times 2^{12} = 519024039293878272000 \approx 5.19 \times 10^{20}$$</p><p>Besides that, representing a Rubik’s cube state in orientation vector ignores cubies’ position conflicting. The total variation is:</p><p>$$24^{20} = 4019988717840603673710821376 \approx 4.02 \times 10^{28}$$</p><p>(For equality, I ignored 6 axes cubies here.)</p><p>And above all these, the facet color scheme allow painting abitrary color in 6 kinds for every facet. Its total variation is:</p><p>$$6^{48} = 22452257707354557240087211123792674816 \approx 2.25 \times 10^{38}$$</p><p>As a Rubik’s cube computer implementation, I’m afraid the redundancy of this scheme is beyond necessary, and is waste and misleading.</p><h2 id="Calculation-of-Rubik’s-cube">Calculation of Rubik’s cube</h2><p>Once we represent a Rubik’s cube state by a matrix, we can calculate it purely by algebra.This is an example to show what the Rubik’s cube multiplication looks like:</p><figure><iframe src="/klstudio/embed.html#/documents/cube3-multiplication-demo" width="800" height="400"></iframe></figure><p>As all groups, Rubik’s cube multiplication obeys associative law, but is not exchangeable.</p><h2 id="Rubik’s-cube-solver">Rubik’s cube solver</h2><p>Now, we see this significant fact: <strong>the Rubik’s cube solver problem is a matrix decomposition problem</strong>!</p><p>Specifically, we have 12 unit quarter twists in matrix form:</p><p>$$\begin{aligned}&amp; U = \left \langle  \alpha \alpha \zeta \zeta \alpha \alpha \zeta \zeta  \alpha \alpha \alpha \alpha \zeta \zeta \zeta \zeta \alpha \alpha \alpha \alpha  \alpha \alpha \zeta \alpha \alpha \alpha  \right \rangle \\&amp; U’ = \left \langle  \alpha \alpha \iota \iota \alpha \alpha \iota \iota  \alpha \alpha \alpha \alpha \iota \iota \iota \iota \alpha \alpha \alpha \alpha  \alpha \alpha \iota \alpha \alpha \alpha  \right \rangle \\&amp; D = \left \langle  \iota \iota \alpha \alpha \iota \iota \alpha \alpha  \iota \iota \iota \iota \alpha \alpha \alpha \alpha \alpha \alpha \alpha \alpha  \alpha \alpha \alpha \iota \alpha \alpha  \right \rangle \\&amp; D’ = \left \langle  \zeta \zeta \alpha \alpha \zeta \zeta \alpha \alpha  \zeta \zeta \zeta \zeta \alpha \alpha \alpha \alpha \alpha \alpha \alpha \alpha  \alpha \alpha \alpha \zeta \alpha \alpha  \right \rangle \\&amp; L = \left \langle  \theta \alpha \theta \alpha \theta \alpha \theta \alpha  \alpha \alpha \theta \alpha \alpha \alpha \theta \alpha \theta \alpha \alpha \theta  \alpha \alpha \alpha \alpha \theta \alpha  \right \rangle \\&amp; L’ = \left \langle  \epsilon \alpha \epsilon \alpha \epsilon \alpha \epsilon \alpha  \alpha \alpha \epsilon \alpha \alpha \alpha \epsilon \alpha \epsilon \alpha \alpha \epsilon  \alpha \alpha \alpha \alpha \epsilon \alpha  \right \rangle \\&amp; R = \left \langle  \alpha \epsilon \alpha \epsilon \alpha \epsilon \alpha \epsilon  \alpha \alpha \alpha \epsilon \alpha \alpha \alpha \epsilon \alpha \epsilon \alpha \epsilon  \alpha \alpha \alpha \alpha \alpha \epsilon  \right \rangle \\&amp; R’ = \left \langle  \alpha \theta \alpha \theta \alpha \theta \alpha \theta  \alpha \alpha \alpha \theta \alpha \alpha \alpha \theta \alpha \theta \alpha \theta  \alpha \alpha \alpha \alpha \alpha \theta  \right \rangle \\&amp; F = \left \langle  \eta \eta \eta \eta \alpha \alpha \alpha \alpha  \eta \alpha \alpha \alpha \eta \alpha \alpha \alpha \eta \eta \alpha \alpha  \eta \alpha \alpha \alpha \alpha \alpha  \right \rangle \\&amp; F’ = \left \langle  \kappa \kappa \kappa \kappa \alpha \alpha \alpha \alpha  \kappa \alpha \alpha \alpha \kappa \alpha \alpha \alpha \kappa \kappa \alpha \alpha  \kappa \alpha \alpha \alpha \alpha \alpha  \right \rangle \\&amp; B = \left \langle  \alpha \alpha \alpha \alpha \kappa \kappa \kappa \kappa  \alpha \kappa \alpha \alpha \alpha \kappa \alpha \alpha \alpha \alpha \kappa \kappa  \alpha \kappa \alpha \alpha \alpha \alpha  \right \rangle \\&amp; B’ = \left \langle  \alpha \alpha \alpha \alpha \eta \eta \eta \eta  \alpha \eta \alpha \alpha \alpha \eta \alpha \alpha \alpha \alpha \eta \eta  \alpha \eta \alpha \alpha \alpha \alpha  \right \rangle \\\end{aligned}$$</p><p>To find a path from the solved state to an arbitrary state, is just finding a multiplication decomposition in unit twists for the specific state matrix.</p><p>We known that any 3-order Rubik’s cube state can be solved in 26 quarter twists in most<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.But finding the shortest solution is still a pending problem.I hope the matrix representation can provide some new approaches for this problem. After all, linear algebra is a highly developed domain already.</p><h2 id="Try-it-youself">Try it youself</h2><figure><iframe src="/klstudio/embed.html#/documents/dynamic-labeled-cube3" width="960" height="600"></iframe><figcaption>Twist this Rubik's cube and see its matrix. <a target="_blank" href="/klstudio/#/documents/dynamic-labeled-cube3">Open in new tab &#x1f855;</a></figcaption></figure><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="http://www.cube20.org/qtm/">God’s Number is 26 in the Quarter-Turn Metric</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/cube3-matrix-tetris.svg&quot; alt=&quot;a cube3 matrix samle #LKONLKONLOKNLKONKLNOGCAAFD&quot; /&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		Guess what this is? &lt;a target=&quot;_blank&quot; href=&quot;/klstudio/#/documents/dynamic-labeled-cube3#LKONLKONLOKNLKONKLNOGCAAFD&quot;&gt;See the answer here.&lt;/a&gt;
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To describe a chess game state, you don’t need to mention every chess piece’s details.
Essentially, a chess piece is just a symbol. Ignoring appearance detail helps us to grab the game gist.
Till now, a popular way to represent a Rubik’s cube state is the facelets expanding graph, like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/cube3-expand-graph-original.png&quot; alt=&quot;a cube3 expanding graph sample&quot;&gt;&lt;/p&gt;
&lt;p&gt;By this way, you can’t tell which facelets are adjacent each other straightway, also it’s hard to imagine what the cube will change into after a twist applied.
That because it comes from the appearance, but not the essential.
Furtherly, as &lt;a href=&quot;/2020/02/05/cube-algebra/#Motivation&quot;&gt;my preivous blog&lt;/a&gt; wrote, Rubik’s Cube solver programs who construct cube state from facelet color is clumsy.
Rubik’s Cube is a game about cubes’s &lt;strong&gt;rotation&lt;/strong&gt; and &lt;strong&gt;permutation&lt;/strong&gt; (but not painting color), &lt;strong&gt;matrix&lt;/strong&gt; is the most proper math tool here.&lt;/p&gt;
&lt;p&gt;However, the fact revealed by this method is not easy to see through, and that is just what I will tell you in this blog.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="algebra" scheme="https://k-l-lambda.github.io/tags/algebra/"/>
    
      <category term="rubiks_cube" scheme="https://k-l-lambda.github.io/tags/rubiks-cube/"/>
    
  </entry>
  
  <entry>
    <title>A Postscript for StyleGAN Mapping Network Geometry Visualization</title>
    <link href="https://k-l-lambda.github.io/2020/04/25/stylegan-mapping-tsne/"/>
    <id>https://k-l-lambda.github.io/2020/04/25/stylegan-mapping-tsne/</id>
    <published>2020-04-25T11:52:06.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><img src="/images/stylegan-mapping-sampling-pca.png" class="figure" width="400" alt="StyleGAN mapping sampling points visualization by embedding projector"></picture><figcaption><p>StyleGAN mapping sampling points visualization by embedding projector</p><p><a href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/k-l-lambda/ec91b00e74a62b6435ec098138f9ab0d/raw/df0e1a3f7a8e29476e30c723038f425f71bba0bd/embedding-projector-config.json">click here to see 3D visulization</a></p></figcaption></figure><p>Some days after the former post of <a href="/2020/02/10/stylegan-mapping/">StyleGAN Mapping Network Geometry Visualization</a>,I realized that there are some canonical dimension reduction methods for data visualization, such as <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>, <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>.These ways may be more intuitive to show data characteristics. So I did some attempts on this.</p><span id="more"></span><p>Firstly I tried a <a href="https://github.com/tensorflow/tfjs-tsne">t-SNE implementation by tensorflow.js</a>,disappointedly, after a moment struggling against release version compatibility problem,I found that the upper limit of data dimensions is merely 40 on a browser WebGL backend, while SytleGAN W space is 512-d.</p><p>Finally, I give up the attempt of a more sophisticated t-SNE implementation,I found the <a href="https://projector.tensorflow.org/">tensorflow embedding projector</a> is a not bad option.Its integration with github gist is handy.</p><p>This is the <a href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/k-l-lambda/ec91b00e74a62b6435ec098138f9ab0d/raw/df0e1a3f7a8e29476e30c723038f425f71bba0bd/embedding-projector-config.json"><strong>live 3D visualization</strong></a>.It seems t-SNE result is more smooth, but a bit unstable.For t-SNE, the result to the experiment of one random circle, points will convergence to a nearly regular round.That seems mainly caused by lack of adjacencies on a circle sampling. Then I made a configuration of 3 random circles, as shown in the top illustration.Most dimension reduction algorithm normalized original data, therefore the bias information is lost. That is a disadvantage.</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
	&lt;picture&gt;
		&lt;img src=&quot;/images/stylegan-mapping-sampling-pca.png&quot; class=&quot;figure&quot; width=&quot;400&quot; alt=&quot;StyleGAN mapping sampling points visualization by embedding projector&quot;&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		&lt;p&gt;StyleGAN mapping sampling points visualization by embedding projector&lt;/p&gt;
		&lt;p&gt;&lt;a href=&quot;https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/k-l-lambda/ec91b00e74a62b6435ec098138f9ab0d/raw/df0e1a3f7a8e29476e30c723038f425f71bba0bd/embedding-projector-config.json&quot;&gt;click here to see 3D visulization&lt;/a&gt;&lt;/p&gt;
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Some days after the former post of &lt;a href=&quot;/2020/02/10/stylegan-mapping/&quot;&gt;StyleGAN Mapping Network Geometry Visualization&lt;/a&gt;,
I realized that there are some canonical dimension reduction methods for data visualization, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&quot;&gt;t-SNE&lt;/a&gt;.
These ways may be more intuitive to show data characteristics. So I did some attempts on this.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="deep_learning" scheme="https://k-l-lambda.github.io/tags/deep-learning/"/>
    
      <category term="stylegan" scheme="https://k-l-lambda.github.io/tags/stylegan/"/>
    
  </entry>
  
  <entry>
    <title>StyleGAN Mapping Network Geometry Visualization</title>
    <link href="https://k-l-lambda.github.io/2020/02/10/stylegan-mapping/"/>
    <id>https://k-l-lambda.github.io/2020/02/10/stylegan-mapping/</id>
    <published>2020-02-10T14:22:52.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><picture><source srcset="/images/stylegan-network.webp" type="image/webp" /><source srcset="/images/stylegan-network.png" type="image/png" /><img src="/images/stylegan-network.png" width="240" /></picture><picture><source srcset="/images/stylegan-geometry.webp" type="image/webp" /><source srcset="/images/stylegan-geometry.jpg" type="image/jpeg" /><img src="/images/stylegan-geometry.jpg" width="600" /></picture><figcaption>StyleGAN generator network architecture & geometry conceptual illustration</figcaption></figure><p><a href="https://github.com/NVlabs/stylegan2">StyleGAN</a><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> generator network has two parts: full-connected mapping network (named <em><code>mapping</code></em>), and pyramid CNN synthesis network (named <em><code>g</code></em>).<code>Mapping</code> is a transformation from dimension 512 to 512, and <code>g</code> is a transformation from dimension 512 to 1024×1024×3.The design of <code>mapping</code> is intended to disentangle the manifold mapping from latent space to feature variation space.I’m interested in how the shape of learned mapping in network warps exactly, so this is my experiment.</p><span id="more"></span><p>By normalization at the beginning of mapping network, input z vectors are on the regular 512-d unit spherical surface.Supposing <code>mapping</code> is a continuous function, all possible w points from Z will distribute on a irregular closed 512-d surface.To show a 512-d manifold is difficult, for humans only have 2-d vision. But we can show some local characteristics by dimension slicing.</p><p>Here is my way. Pick a geodesic line on sphere, i.e. a great circle, map it into W space, then show the warped result circle.To get a great circle of 512-d sphere, for generility, random sample 2 points (by a standard normal distribution sample then normalize it),then slerp between and beyond them multiple times evenly, until finished one cycle on the sphere.To show the 512-d result circle, I simply project the high dimensional line into multiple low dimensional lines.I.e. for every point <em>w</em> in the result circle:</p><p>$$ \textbf{w}: [w_{1}, w_{2}, w_{3}, …, w_{512}] \rightarrow \{[w_{1}, w_{2}, w_{3}], [w_{4}, w_{5}, w_{6}], …\} $$</p><p>Then plot the projections in a 3D coordinate system viewport, as you see below.</p><figure><span class="fixed-ratio" style="width: 100%; padding-top: 66%; padding-top: min(66%, 586px); max-width: 1025px"><iframe src="/klstudio/embed.html#/documents/stylegan-mapping"></iframe></span><figcaption>Evenly interpolated 96 points on a great circle of unit sphere. <br />Sample circles: 1 specified (on the plane of first 2 axes) and 5 random. <br />The mapping network is from <em>stylegan2-ffhq-config-f</em><sup><a target="_blank" href="https://github.com/NVlabs/stylegan2/blob/master/pretrained_networks.py#L32">source</a></sup>.</figcaption></figure><p>As you see in the plotting, projected circles entwines in most dimensions. So the mapping from Z to W is more rugged than I expected in the conceptual illustration.Intervals between neighbor points, though not very even, but high dimensional gauge can’t be speculated by low dimensional projections.</p><p>When you select very many dimensions (by moving the second slider to right), you will see the overall distribution of points’ coordinates.It may be a significant observation that most points congregate at the first octant (+, +, +), more exactly, the tetrahedron area with vertices about <em>(0, 0, 0), (1.5, 0, 0), (0, 1.5, 0), (0, 0, 1.5)</em>.This phenomenon reminds <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> activation’s effection.According to StyleGAN source code<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, <em>Leaky ReLU</em> is used in mapping network by default, which coincides the ploting results.In some sense, the asymmetry may be necessary to disentanglement learning.But in a further thinking, considering the network is trained on a dataset from nature, why nature need such a specific asymmetry and where it come from?</p><p>Lastly, inspection on features of generated images. Let’s suppose there are some superplanes in the Z space, which split some binary high-level semantic features,such as male/female, young/old, skin color dark/light and so on (for some feature there is no definite boundary probably, but moving along some direction, i.e. plane’s normal vector, will change this feature most rapidly)<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>.And we can safely suppose that a random great circle (with a random normal vector) on unit sphere will intersect with most feature superplanes.In fact, considering the high dimensions, 2 random superplanes will be very closed to perpendicular in most cases.So we will get an interesting inference, generated images sampling from a great circle will experience many features variation: male/female, old/young, and anything else you can imagine.So such an experiment can be helpful to see the diversity of a GAN, and test how well fitted the network with dataset.</p><p>This is my StyleGAN <a href="https://github.com/k-l-lambda/stylegan-web">web porting project</a> for research. A video demo:</p><a href="https://github.com/k-l-lambda/stylegan-web"><video src="/images/explorer-demo.webm" style="width: 100%; max-width: 800px" autoplay loop></video></a><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>paper: <a href="https://arxiv.org/abs/1812.04948">A Style-Based Generator Architecture for Generative Adversarial Networks</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>paper: <a href="https://arxiv.org/abs/1912.04958">Analyzing and Improving the Image Quality of StyleGAN</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p><a href="https://github.com/NVlabs/stylegan2/blob/master/training/networks_stylegan2.py#L261">mapping network code</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Someone may argue that feature space could be more straight for W than Z, but considering the highly irregular shape and the relation between <em>ψ</em> and feature intensity, I think it’s an open question. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
	&lt;picture&gt;
		&lt;source srcset=&quot;/images/stylegan-network.webp&quot; type=&quot;image/webp&quot; /&gt;
		&lt;source srcset=&quot;/images/stylegan-network.png&quot; type=&quot;image/png&quot; /&gt;
		&lt;img src=&quot;/images/stylegan-network.png&quot; width=&quot;240&quot; /&gt;
	&lt;/picture&gt;
	&lt;picture&gt;
		&lt;source srcset=&quot;/images/stylegan-geometry.webp&quot; type=&quot;image/webp&quot; /&gt;
		&lt;source srcset=&quot;/images/stylegan-geometry.jpg&quot; type=&quot;image/jpeg&quot; /&gt;
		&lt;img src=&quot;/images/stylegan-geometry.jpg&quot; width=&quot;600&quot; /&gt;
	&lt;/picture&gt;
	&lt;figcaption&gt;
		StyleGAN generator network architecture &amp; geometry conceptual illustration
	&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/NVlabs/stylegan2&quot;&gt;StyleGAN&lt;/a&gt;&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn1&quot; id=&quot;fnref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt; &lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn2&quot; id=&quot;fnref2&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt; generator network has two parts: full-connected mapping network (named &lt;em&gt;&lt;code&gt;mapping&lt;/code&gt;&lt;/em&gt;), and pyramid CNN synthesis network (named &lt;em&gt;&lt;code&gt;g&lt;/code&gt;&lt;/em&gt;).
&lt;code&gt;Mapping&lt;/code&gt; is a transformation from dimension 512 to 512, and &lt;code&gt;g&lt;/code&gt; is a transformation from dimension 512 to 1024×1024×3.
The design of &lt;code&gt;mapping&lt;/code&gt; is intended to disentangle the manifold mapping from latent space to feature variation space.
I’m interested in how the shape of learned mapping in network warps exactly, so this is my experiment.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="deep_learning" scheme="https://k-l-lambda.github.io/tags/deep-learning/"/>
    
      <category term="stylegan" scheme="https://k-l-lambda.github.io/tags/stylegan/"/>
    
  </entry>
  
  <entry>
    <title>Cube Rotation Algebra</title>
    <link href="https://k-l-lambda.github.io/2020/02/05/cube-algebra/"/>
    <id>https://k-l-lambda.github.io/2020/02/05/cube-algebra/</id>
    <published>2020-02-05T17:00:57.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<figure><span class="max600"><span class="fixed-ratio" style="width: 100%; padding-top: 100%"><iframe src="/klstudio/embed.html#/documents/mesh-viewer-demo:quarter-array-4x6"></iframe></span></span><figcaption>In 3D space, an object has 6 &times; 4 = 24 orthogonal orientations in total.</figcaption></figure><h2 id="Motivation">Motivation</h2><p>Canonical Rubik’s Cube solver algorithm<sup><a href="https://github.com/hkociemba/RubiksCube-TwophaseSolver">1</a></sup> constructs cube state from face colors and a lot of permutation rules. That may waste too many coding :)Face color is merely appearance, cubies’ orientation is essential.Since Rubik’s Cube seems already been used as the avatar of group theory (check this <a href="https://en.wikipedia.org/wiki/Group_theory">wikipedia entry</a>),it’s better to clarify all details of the cube rotation group structure, and construct the whole Rubik’s Cube representations based on cubies’ orientation.</p><p>Regardless of Rubik’s Cube, orthogonal rotation in 3D space is usual and connected with interesting problems. E.g. how to quickly tell if 2 orthogonal <a href="https://en.wikipedia.org/wiki/Euler_angles">Euler angles</a> are the same rotation, purely by algebra without experiment?<a href="https://en.wikipedia.org/wiki/Quaternion">Quaternion calculus</a> may be a short answer, but when you do that, irrational numbers are inevitable, and that seems wasting and precise problematic.</p><p>Programmers prefer easy implementation, which based on a set of simple representation and rules applied to them. No float numbers, no redundancy.All you need is a multiplication table, and the table is highly symmetric, so let’s begin from analyzing the <strong>R</strong><sup>3</sup> orthogonal rotation group structure.</p><span id="more"></span><h2 id="Quaternion-space-half-reduction">Quaternion space half reduction</h2><p>In fact, we can invent a new symbol system to represent every element, but it may be a better choice to keep compatible with quaternion.However, by quaternion we should solve an ambiguity issue firstly. Because in <strong>R</strong><sup>3</sup> geometry, a pair of quaternion <em>±q</em> represent a same rotation.We need an assistant function to reduce redundancy.</p><p>We define a space half reduction sign function <strong>sgn<sub>h</sub></strong>:</p><p>$$sgn_{h}(a+bi+cj+dk) = \begin{cases}1 &amp; \text{ if } a&gt;0, \\&amp; \text{ or } a=0, b&gt;0, \\&amp; \text{ or } a=b=0, c&gt;0, \\&amp; \text{ or } a=b=c=0, d&gt;0, \\\\0 &amp; \text{ if } a=b=c=d=0, \\\\-1 &amp; \text{ otherwise. }\end{cases}$$</p><p>Then define the space half reduction absolute function <strong>abs<sub>h</sub></strong>:</p><p>$$abs_{h}(q) = sgn_{h}(q) \cdot q$$</p><p>In the quaternion <strong>R</strong><sup>4</sup> space, this function reflect (by centrosymmetric) a half space to the other side.And right on the reflection interface <strong>R</strong><sup>3</sup> subspace, do the same thing, and also for <strong>R</strong><sup>2</sup> interface, <strong>R</strong><sup>1</sup> interface recursively.</p><p>Then we define a shorthand representation</p><p>$$q \cong p$$</p><p>to stand for:</p><p>$$abs_{h}(q) = abs_{h}( p ),$$</p><p>which means <em>q = p</em> or <em>q = -p</em>, i.e. they represent the same <strong>R</strong><sup>3</sup> orientation/rotation. Obviously, this <em>half-reduction equal</em> also has transitivity as plain equal.</p><h2 id="Calculus">Calculus</h2><p>Before calculus, here are tips for readers unfamiliar with quaternion:</p><ul><li>imagine units <em>i</em>, <em>j</em>, <em>k</em> stand for 180° rotation along 3 axes in <strong>R</strong><sup>3</sup> respectively;</li><li>quaternion multiplication stands for rotations concatenation (not exchangable);</li><li>square root of imagine unit stands for 90° rotation;</li><li>q<sup>-1</sup> stands for the opponent rotation of q.</li></ul><p>Now we have these obvious facts:</p><p>$$i^{2} \cong j^{2} \cong k^{2} \cong 1$$</p><p>means 360° rotation returns to origin.</p><p>$$i \cong i^{-1} \\j \cong j^{-1} \\k \cong k^{-1}$$</p><p>means ±180° rotation (along a same axis) arrives the same orientation.</p><p>$$i^{\frac{3}{2}} \cong i^{-\frac{1}{2}} \\j^{\frac{3}{2}} \cong j^{-\frac{1}{2}} \\k^{\frac{3}{2}} \cong k^{-\frac{1}{2}}$$</p><p>means 270° rotation equal -90° rotation (along a same axis).</p><p>Not very obvious, but we can get rest items by computaion. The twice heterogeneous quarter rotation (can also be treated as 120° rotation along a cube diagonal):</p><p>$$\sqrt{i}\sqrt{j} = \sqrt{j}\sqrt{k} = \sqrt{k}\sqrt{i} \\\sqrt{i}\sqrt[-]{j} = \sqrt[-]{j}\sqrt[-]{k} = \sqrt[-]{k}\sqrt{i} \\\sqrt[-]{i}\sqrt{j} = \sqrt{j}\sqrt[-]{k} = \sqrt[-]{k}\sqrt[-]{i} \\\sqrt[-]{i}\sqrt[-]{j} = \sqrt[-]{j}\sqrt{k} = \sqrt[-]{i}\sqrt{k}$$</p><p>$$\sqrt{j}\sqrt{i} = \sqrt[-]{k}\sqrt{j} = \sqrt{i}\sqrt[-]{k} \\\sqrt[-]{j}\sqrt{i} = \sqrt{k}\sqrt[-]{j} = \sqrt{i}\sqrt{k} \\\sqrt{j}\sqrt[-]{i} = \sqrt{k}\sqrt{j} = \sqrt[-]{i}\sqrt{k} \\\sqrt[-]{j}\sqrt[-]{i} = \sqrt[-]{k}\sqrt[-]{j} = \sqrt[-]{i}\sqrt[-]{k}$$</p><p>Which $\sqrt[-]{x}$ stand for $x^{-\frac{1}{2}}$.</p><p>(Wait, does $\sqrt{i}$ make sense? Yes, $\sqrt{i} = \frac{\sqrt{2}}{2}(1 + i)$, try to do this math: $(\frac{\sqrt{2}}{2}(1 + i))^2$.)</p><p>These 8 lines don’t equal each other (notice that quaternion multiplication is not exchangable, so $\sqrt{i}\sqrt{j} \neq \sqrt{j}\sqrt{i}$).In fact, in additonal form, they are values of all the permutations among <em>0.5±0.5i±0.5j±0.5k</em>.</p><p>Look at these equations carefully, they are highly symmetric. For i-j-k cycle, <em>minus</em> is even, for k-j-i cycle, <em>minus</em> is odds.Based on these rules, we can do most of algebra deductions to simplify a complex mutiplication expression.We can call this kind of symmetry <em>ternary permutability</em>. And I think ternary permutability can be treated as the core of quaternion.</p><p>And thrice quarter rotation (can also be treated as 180° rotation along a section square diagonal):</p><p>$$\sqrt{i}j = \sqrt[-]{i}k = j\sqrt[-]{i} = k\sqrt[]{i} \\\sqrt[-]{i}j = \sqrt[]{i}k = j\sqrt[]{i} = k\sqrt[-]{i} \\\sqrt{j}k = \sqrt[-]{j}i = k\sqrt[-]{j} = i\sqrt[]{j} \\\sqrt[-]{j}k = \sqrt[]{j}i = k\sqrt[]{j} = i\sqrt[-]{j} \\\sqrt{k}i = \sqrt[-]{k}j = i\sqrt[-]{k} = j\sqrt[]{k} \\\sqrt[-]{k}i = \sqrt[]{k}j = i\sqrt[]{k} = j\sqrt[-]{k}$$</p><p>These 6 lines, plus basic $±\sqrt{i}$, $±\sqrt{j}$, $±\sqrt{k}$, these 12 items are values of all reposition permutations of $(\frac{\sqrt{2}}{2}, ±\frac{\sqrt{2}}{2}, 0, 0)$ multipy by $(1, i, j, k)^{T}$,which $\frac{\sqrt{2}}{2}$ should be prior than $±\frac{\sqrt{2}}{2}$ to satisfy space half-reduction.Because these items’ order number is $\frac{1}{2}$ or $\frac{3}{2}$, we call them <em>odds</em> items, correspondingly, items with order number 0 or 1, called <em>even</em> items.</p><p>Now this is an example to show orthogonal rotation combination simplification, purely by symbol replacement and avoid number calculations.(Also an answer to Euler angles problem mentioned ealier.)</p><p>$$\begin{aligned}\sqrt{i}\sqrt{j}\sqrt{k}\sqrt{i}\sqrt{j}\sqrt{k} &amp;= \sqrt{i}(\sqrt{\textbf{j}}\sqrt{\textbf{k}})\sqrt{i}\sqrt{j}\sqrt{k} \\&amp; = \sqrt{i}(\sqrt{i}\sqrt{j})\sqrt{i}\sqrt{j}\sqrt{k} \\&amp; = \sqrt{i}\sqrt{i}(\sqrt{\textbf{j}}\sqrt{\textbf{i}})\sqrt{j}\sqrt{k} \\&amp; = \sqrt{i}\sqrt{i}(\sqrt{i}\sqrt[-]{k})\sqrt{j}\sqrt{k} \\&amp; = \sqrt{i}\sqrt{i}\sqrt{i}(\sqrt[-]{\textbf{k}}\sqrt{\textbf{j}})\sqrt{k} \\&amp; = \sqrt{i}\sqrt{i}\sqrt{i}(\sqrt{i}\sqrt[-]{k})\sqrt{k} \\&amp; = (\sqrt{i}\sqrt{i}\sqrt{i}\sqrt{i})(\sqrt[-]{k}\sqrt{k}) \\&amp; = i^{2} \cdot 1 \\&amp; = -1 \cong 1\end{aligned}$$</p><h2 id="Elements-and-group">Elements and group</h2><p>Enumerated all possible combinations, we have all 24 individual elements of group. In additional form, they can be listed as:</p><table><thead><tr><th>$\cdot (1, i, j, k)^{T}$</th><th>how many items</th><th></th></tr></thead><tbody><tr><td>1, 0, 0, 0</td><td>4</td><td><em>even</em></td></tr><tr><td>$\frac{\sqrt{2}}{2}$, $±\frac{\sqrt{2}}{2}$, 0, 0</td><td>12</td><td><em>odds</em></td></tr><tr><td>0.5, ±0.5, ±0.5, ±0.5</td><td>8</td><td><em>even</em></td></tr></tbody></table><p>Which 0s’ position in tuples are arbitrary.</p><p>Though addtional form has advantage of unique form for every element, but long for written, and identification confusable.So I prefer to use multiplication form, and which is consistent with Euler angle, therefore geometry instinct and easy to comprehend.To reduce redundancy items in multiplication form, I picks item by alphabetical order, and in the same letter, by order of $\sqrt{i}$, $\sqrt[-]{i}$, $i$.</p><p>Then we get the 24 elements set:</p><p>$$ O_{24}: \{ 1, \sqrt{i}, \sqrt[-]{i}, \sqrt{j}, \sqrt[-]{j}, \sqrt{k}, \sqrt[-]{k}, i, j, k, \sqrt{i}\sqrt{j}, \sqrt{i}\sqrt[-]{j}, \sqrt{i}\sqrt{k}, \sqrt{i}\sqrt[-]{k}, \sqrt[-]{i}\sqrt{j}, \sqrt[-]{i}\sqrt[-]{j}, \sqrt[-]{i}\sqrt{k}, \sqrt[-]{i}\sqrt[-]{k}, \sqrt{i}j, \sqrt[-]{i}j, i\sqrt{j}, i\sqrt[-]{j}, i\sqrt{k}, i\sqrt[-]{k} \} $$</p><p>And categorization by distance from origin:</p><table><thead><tr><th style="text-align:center">elements</th><th>how many quarters rotations</th><th></th></tr></thead><tbody><tr><td style="text-align:center">1</td><td>0</td><td>identity</td></tr><tr><td style="text-align:center">$\sqrt{i}, \sqrt[-]{i}, \sqrt{j}, \sqrt[-]{j}, \sqrt{k}, \sqrt[-]{k}$</td><td>1</td><td>one quarter</td></tr><tr><td style="text-align:center">$i, j, k$</td><td>2</td><td>half</td></tr><tr><td style="text-align:center">$\sqrt{i}\sqrt{j}, \sqrt{i}\sqrt[-]{j}, \sqrt[-]{i}\sqrt{j}, \sqrt[-]{i}\sqrt[-]{j}, \sqrt{i}\sqrt{k}, \sqrt{i}\sqrt[-]{k}, \sqrt[-]{i}\sqrt{k}, \sqrt[-]{i}\sqrt[-]{k}$</td><td>2</td><td>two quaters</td></tr><tr><td style="text-align:center">$\sqrt{i}j, \sqrt[-]{i}j, i\sqrt{j}, i\sqrt[-]{j}, i\sqrt{k}, i\sqrt[-]{k}$</td><td>3</td><td>three quarters</td></tr></tbody></table><p>The visualization:</p><figure class="fixed-ratio" style="width: 100%; padding-top: 67%"><iframe src="/klstudio/embed.html#/documents/mesh-viewer-demo:quarter-categories"></iframe></figure><p>Now only one more thing, define a half-reduction multiplication as operation:</p><p>$$ q \otimes p := abs_{h}(q \cdot p) $$</p><p>$O_{24}$ is closed for this operation, i.e. all results by half-reduction multipy between 2 arbitrary elements in $O_{24}$ are returned in 24 elements.</p><p>$1$ is the identity element, $\{ \sqrt{i}, \sqrt{j}, \sqrt{k} \}$ is the generating set, i.e. all 24 elements can be generated by multiplication among this 3 elements.Every element has a corresponding inverse element.</p><p>Then we get the cube symmetry group (or <a href="https://en.wikipedia.org/wiki/Octahedral_symmetry#Full_octahedral_symmetry">full octahedral symmetry group</a>):</p><p>$$ O_{h}: \{ O_{24}, 1, \otimes \} $$</p><p>This is the group table:</p><p style="font-size: 9px">$\begin{matrix}\otimes & \textbf{1} & \textbf{i} & \textbf{j} & \textbf{k} & \sqrt{\textbf{i}} & \sqrt{\textbf{j}} & \sqrt{\textbf{k}} & \sqrt[-]{\textbf{i}} & \sqrt[-]{\textbf{j}} & \sqrt[-]{\textbf{k}} & \sqrt{\textbf{i}}\sqrt{\textbf{j}} & \sqrt{\textbf{i}}\sqrt[-]{\textbf{j}} & \sqrt{\textbf{i}}\sqrt{\textbf{k}} & \sqrt{\textbf{i}}\sqrt[-]{\textbf{k}} & \sqrt[-]{\textbf{i}}\sqrt{\textbf{j}} & \sqrt[-]{\textbf{i}}\sqrt[-]{\textbf{j}} & \sqrt[-]{\textbf{i}}\sqrt{\textbf{k}} & \sqrt[-]{\textbf{i}}\sqrt[-]{\textbf{k}} & \sqrt{\textbf{i}}\textbf{j} & \sqrt[-]{\textbf{i}}\textbf{j} & \textbf{i}\sqrt{\textbf{j}} & \textbf{i}\sqrt[-]{\textbf{j}} & \textbf{i}\sqrt{\textbf{k}} & \textbf{i}\sqrt[-]{\textbf{k}} \\\\\textbf{1}  & 1 & i & j & k & \sqrt{i} & \sqrt{j} & \sqrt{k} & \sqrt[-]{i} & \sqrt[-]{j} & \sqrt[-]{k} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt{i}j & \sqrt[-]{i}j & i\sqrt{j} & i\sqrt[-]{j} & i\sqrt{k} & i\sqrt[-]{k} \\\\\textbf{i}  & i & 1 & k & j & \sqrt[-]{i} & i\sqrt{j} & i\sqrt{k} & \sqrt{i} & i\sqrt[-]{j} & i\sqrt[-]{k} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}j & \sqrt{i}j & \sqrt{j} & \sqrt[-]{j} & \sqrt{k} & \sqrt[-]{k} \\\\\textbf{j}  & j & k & 1 & i & \sqrt[-]{i}j & \sqrt[-]{j} & i\sqrt[-]{k} & \sqrt{i}j & \sqrt{j} & i\sqrt{k} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i} & \sqrt{i} & i\sqrt[-]{j} & i\sqrt{j} & \sqrt[-]{k} & \sqrt{k} \\\\\textbf{k}  & k & j & i & 1 & \sqrt{i}j & i\sqrt[-]{j} & \sqrt[-]{k} & \sqrt[-]{i}j & i\sqrt{j} & \sqrt{k} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{j} & \sqrt{i} & \sqrt[-]{i} & \sqrt[-]{j} & \sqrt{j} & i\sqrt[-]{k} & i\sqrt{k} \\\\\sqrt{\textbf{i}}  & \sqrt{i} & \sqrt[-]{i} & \sqrt{i}j & \sqrt[-]{i}j & i & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt{j} & 1 & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt[-]{j} & i\sqrt{j} & i\sqrt[-]{j} & \sqrt{j} & \sqrt[-]{j} & i\sqrt{k} & i\sqrt[-]{k} & \sqrt{k} & \sqrt[-]{k} & k & j & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{k} \\\\\sqrt{\textbf{j}}  & \sqrt{j} & i\sqrt[-]{j} & \sqrt[-]{j} & i\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & j & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt{k} & 1 & \sqrt{i}\sqrt{k} & i\sqrt[-]{k} & \sqrt[-]{k} & i\sqrt{k} & \sqrt{k} & \sqrt{i} & \sqrt[-]{i}j & \sqrt{i}j & \sqrt[-]{i} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & i & k & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt[-]{k} \\\\\sqrt{\textbf{k}}  & \sqrt{k} & i\sqrt[-]{k} & i\sqrt{k} & \sqrt[-]{k} & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt{k} & k & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & 1 & \sqrt{i}j & \sqrt{i} & \sqrt[-]{i} & \sqrt[-]{i}j & i\sqrt{j} & \sqrt{j} & i\sqrt[-]{j} & \sqrt[-]{j} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt[-]{j} & i & j \\\\\sqrt[-]{\textbf{i}}  & \sqrt[-]{i} & \sqrt{i} & \sqrt[-]{i}j & \sqrt{i}j & 1 & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt{k} & i & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt{j} & \sqrt[-]{j} & i\sqrt{j} & i\sqrt[-]{j} & \sqrt{k} & \sqrt[-]{k} & i\sqrt{k} & i\sqrt[-]{k} & j & k & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} \\\\\sqrt[-]{\textbf{j}}  & \sqrt[-]{j} & i\sqrt{j} & \sqrt{j} & i\sqrt[-]{j} & \sqrt[-]{i}\sqrt{j} & 1 & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt[-]{k} & j & \sqrt{i}\sqrt[-]{j} & \sqrt{k} & i\sqrt{k} & \sqrt[-]{k} & i\sqrt[-]{k} & \sqrt[-]{i}j & \sqrt{i} & \sqrt[-]{i} & \sqrt{i}j & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{j} & k & i & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt{j} \\\\\sqrt[-]{\textbf{k}}  & \sqrt[-]{k} & i\sqrt{k} & i\sqrt[-]{k} & \sqrt{k} & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt[-]{j} & 1 & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{k} & k & \sqrt{i} & \sqrt{i}j & \sqrt[-]{i}j & \sqrt[-]{i} & \sqrt[-]{j} & i\sqrt[-]{j} & \sqrt{j} & i\sqrt{j} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt{k} & j & i \\\\\sqrt{\textbf{i}}\sqrt{\textbf{j}}  & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{k} & i\sqrt[-]{k} & \sqrt{i}j & i\sqrt{j} & \sqrt{k} & \sqrt{i} & \sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt{j} & i & j & k & 1 & \sqrt[-]{k} & i\sqrt{k} & \sqrt[-]{i} & \sqrt[-]{i}j & i\sqrt[-]{j} & \sqrt[-]{j} \\\\\sqrt{\textbf{i}}\sqrt[-]{\textbf{j}}  & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{k} & i\sqrt{k} & \sqrt{i} & \sqrt[-]{j} & \sqrt[-]{k} & \sqrt{i}j & i\sqrt[-]{j} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt[-]{k} & j & i & 1 & k & \sqrt{k} & i\sqrt[-]{k} & \sqrt[-]{i}j & \sqrt[-]{i} & \sqrt{j} & i\sqrt{j} \\\\\sqrt{\textbf{i}}\sqrt{\textbf{k}}  & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{j} & \sqrt[-]{k} & \sqrt[-]{i}j & \sqrt{j} & i\sqrt{k} & \sqrt[-]{i} & i\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt{k} & 1 & k & j & i & i\sqrt[-]{k} & \sqrt{k} & \sqrt{i} & \sqrt{i}j & \sqrt[-]{j} & i\sqrt[-]{j} \\\\\sqrt{\textbf{i}}\sqrt[-]{\textbf{k}}  & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{j} & \sqrt{k} & \sqrt[-]{i} & i\sqrt[-]{j} & i\sqrt[-]{k} & \sqrt[-]{i}j & \sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt[-]{j} & k & 1 & i & j & i\sqrt{k} & \sqrt[-]{k} & \sqrt{i}j & \sqrt{i} & i\sqrt{j} & \sqrt{j} \\\\\sqrt[-]{\textbf{i}}\sqrt{\textbf{j}}  & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{j} & i\sqrt{j} & \sqrt{k} & \sqrt[-]{i}j & \sqrt[-]{j} & i\sqrt{k} & \sqrt{i} & k & i & 1 & j & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt[-]{j} & i\sqrt[-]{j} & \sqrt{j} & \sqrt[-]{k} & i\sqrt[-]{k} & \sqrt[-]{i} & \sqrt{i}j \\\\\sqrt[-]{\textbf{i}}\sqrt[-]{\textbf{j}}  & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & i\sqrt[-]{j} & i\sqrt[-]{k} & \sqrt{i} & \sqrt{j} & \sqrt[-]{k} & \sqrt[-]{i}j & i & k & j & 1 & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt{k} & i\sqrt{j} & \sqrt[-]{j} & i\sqrt{k} & \sqrt{k} & \sqrt{i}j & \sqrt[-]{i} \\\\\sqrt[-]{\textbf{i}}\sqrt{\textbf{k}}  & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt{j} & i\sqrt{k} & \sqrt{i}j & i\sqrt[-]{j} & \sqrt{k} & \sqrt[-]{i} & j & 1 & i & k & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{j} & i\sqrt{j} & i\sqrt[-]{k} & \sqrt[-]{k} & \sqrt{i} & \sqrt[-]{i}j \\\\\sqrt[-]{\textbf{i}}\sqrt[-]{\textbf{k}}  & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{j} & \sqrt[-]{k} & \sqrt[-]{i} & i\sqrt{j} & i\sqrt[-]{k} & \sqrt{i}j & 1 & j & k & i & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt{j} & \sqrt{j} & i\sqrt[-]{j} & \sqrt{k} & i\sqrt{k} & \sqrt[-]{i}j & \sqrt{i} \\\\\sqrt{\textbf{i}}\textbf{j}  & \sqrt{i}j & \sqrt[-]{i}j & \sqrt{i} & \sqrt[-]{i} & j & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt[-]{k} & k & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{j} & \sqrt{j} & i\sqrt[-]{j} & i\sqrt{j} & i\sqrt[-]{k} & i\sqrt{k} & \sqrt[-]{k} & \sqrt{k} & 1 & i & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{j} \\\\\sqrt[-]{\textbf{i}}\textbf{j}  & \sqrt[-]{i}j & \sqrt{i}j & \sqrt[-]{i} & \sqrt{i} & k & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt[-]{j} & j & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt{j} & i\sqrt[-]{j} & i\sqrt{j} & \sqrt[-]{j} & \sqrt{j} & \sqrt[-]{k} & \sqrt{k} & i\sqrt[-]{k} & i\sqrt{k} & i & 1 & \sqrt{i}\sqrt[-]{j} & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt{k} \\\\\textbf{i}\sqrt{\textbf{j}}  & i\sqrt{j} & \sqrt[-]{j} & i\sqrt[-]{j} & \sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & k & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt{j} & i & \sqrt{i}\sqrt{j} & \sqrt[-]{k} & i\sqrt[-]{k} & \sqrt{k} & i\sqrt{k} & \sqrt[-]{i} & \sqrt{i}j & \sqrt[-]{i}j & \sqrt{i} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & 1 & j & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt[-]{j} \\\\\textbf{i}\sqrt[-]{\textbf{j}}  & i\sqrt[-]{j} & \sqrt{j} & i\sqrt{j} & \sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & i & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt[-]{j} & k & \sqrt{i}\sqrt[-]{k} & i\sqrt{k} & \sqrt{k} & i\sqrt[-]{k} & \sqrt[-]{k} & \sqrt{i}j & \sqrt[-]{i} & \sqrt{i} & \sqrt[-]{i}j & \sqrt[-]{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{k} & j & 1 & \sqrt{i}\sqrt{j} & \sqrt{i}\sqrt{k} \\\\\textbf{i}\sqrt{\textbf{k}}  & i\sqrt{k} & \sqrt[-]{k} & \sqrt{k} & i\sqrt[-]{k} & \sqrt{i}\sqrt{k} & \sqrt[-]{i}\sqrt{j} & j & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & i & \sqrt[-]{i}j & \sqrt[-]{i} & \sqrt{i} & \sqrt{i}j & \sqrt{j} & i\sqrt{j} & \sqrt[-]{j} & i\sqrt[-]{j} & \sqrt{i}\sqrt[-]{k} & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt[-]{k} & 1 & k \\\\\textbf{i}\sqrt[-]{\textbf{k}}  & i\sqrt[-]{k} & \sqrt{k} & \sqrt[-]{k} & i\sqrt{k} & \sqrt{i}\sqrt[-]{k} & \sqrt[-]{i}\sqrt[-]{k} & i & \sqrt{i}\sqrt{j} & \sqrt[-]{i}\sqrt[-]{j} & j & \sqrt[-]{i} & \sqrt[-]{i}j & \sqrt{i}j & \sqrt{i} & i\sqrt[-]{j} & \sqrt[-]{j} & i\sqrt{j} & \sqrt{j} & \sqrt{i}\sqrt{k} & \sqrt{i}\sqrt[-]{j} & \sqrt[-]{i}\sqrt{k} & \sqrt[-]{i}\sqrt{j} & k & 1\end{matrix}$</p><p>You may notice that top-left 4×4 area in the table is a subgroup by only half rotation elements.</p><p>3D <a href="https://en.wikipedia.org/wiki/Cayley_graph">Cayley graph</a> for the group:</p><figure><span class="fixed-ratio" style="width: 100%; padding-top: 60%"><iframe src="/klstudio/embed.html#/cube-cayley-graph"></iframe></span><figCaption><p>Cayley graph of $O_{h}$, click top right controls to perform permutations.</p><p>This is an ealier work, sorry for I was using <em>i</em>, <em>i'</em> stand for $\sqrt[&pm;]{i}$ in this article.</p></figCaption></figure><p>I have to confess this graph’s architecture configuration is far from perfection, any good idea about $O_{h}$ visualization please tell me.</p><h2 id="Next-step">Next step</h2><p>Soon later, I will talk about some thinking about Rubik’s Cube representation in computer and some ideas maybe helpful for solver algorithm<sup><a href="https://en.wikipedia.org/wiki/God%27s_algorithm">2</a></sup>.</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
	&lt;span class=&quot;max600&quot;&gt;
		&lt;span class=&quot;fixed-ratio&quot; style=&quot;width: 100%; padding-top: 100%&quot;&gt;
			&lt;iframe src=&quot;/klstudio/embed.html#/documents/mesh-viewer-demo:quarter-array-4x6&quot;&gt;&lt;/iframe&gt;
		&lt;/span&gt;
	&lt;/span&gt;
	&lt;figcaption&gt;In 3D space, an object has 6 &amp;times; 4 = 24 orthogonal orientations in total.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;Motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Canonical Rubik’s Cube solver algorithm&lt;sup&gt;&lt;a href=&quot;https://github.com/hkociemba/RubiksCube-TwophaseSolver&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; constructs cube state from face colors and a lot of permutation rules. That may waste too many coding :)
Face color is merely appearance, cubies’ orientation is essential.
Since Rubik’s Cube seems already been used as the avatar of group theory (check this &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_theory&quot;&gt;wikipedia entry&lt;/a&gt;),
it’s better to clarify all details of the cube rotation group structure, and construct the whole Rubik’s Cube representations based on cubies’ orientation.&lt;/p&gt;
&lt;p&gt;Regardless of Rubik’s Cube, orthogonal rotation in 3D space is usual and connected with interesting problems. E.g. how to quickly tell if 2 orthogonal &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler_angles&quot;&gt;Euler angles&lt;/a&gt; are the same rotation, purely by algebra without experiment?
&lt;a href=&quot;https://en.wikipedia.org/wiki/Quaternion&quot;&gt;Quaternion calculus&lt;/a&gt; may be a short answer, but when you do that, irrational numbers are inevitable, and that seems wasting and precise problematic.&lt;/p&gt;
&lt;p&gt;Programmers prefer easy implementation, which based on a set of simple representation and rules applied to them. No float numbers, no redundancy.
All you need is a multiplication table, and the table is highly symmetric, so let’s begin from analyzing the &lt;strong&gt;R&lt;/strong&gt;&lt;sup&gt;3&lt;/sup&gt; orthogonal rotation group structure.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="math" scheme="https://k-l-lambda.github.io/tags/math/"/>
    
      <category term="algebra" scheme="https://k-l-lambda.github.io/tags/algebra/"/>
    
      <category term="group_theory" scheme="https://k-l-lambda.github.io/tags/group-theory/"/>
    
      <category term="rubiks_cube" scheme="https://k-l-lambda.github.io/tags/rubiks-cube/"/>
    
  </entry>
  
  <entry>
    <title>Embedding Test</title>
    <link href="https://k-l-lambda.github.io/2020/02/03/embed-test/"/>
    <id>https://k-l-lambda.github.io/2020/02/03/embed-test/</id>
    <published>2020-02-03T16:17:45.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<iframe src="/klstudio/embed.html" style="border: 0; width: 100%; height: calc(min(60vh, 60vw))"></iframe><p><em>This is just a page embedding test. Have fun.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;iframe src=&quot;/klstudio/embed.html&quot; style=&quot;border: 0; width: 100%; height: calc(min(60vh, 60vw))&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;em&gt;This is just a page embedd
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>The First Post</title>
    <link href="https://k-l-lambda.github.io/2020/02/01/first-post/"/>
    <id>https://k-l-lambda.github.io/2020/02/01/first-post/</id>
    <published>2020-02-01T14:25:11.000Z</published>
    <updated>2025-04-23T16:01:47.649Z</updated>
    
    <content type="html"><![CDATA[<p>Blog open, this is the first post.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Blog open, this is the first post.&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
</feed>
